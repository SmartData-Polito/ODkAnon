{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef70031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import folium\n",
    "import hdbscan\n",
    "import h3\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pl\n",
    "import matplotlib.patches as patches\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from typing import Dict, Set, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a2450-7ab8-4403-8737-0e3237f62a55",
   "metadata": {},
   "source": [
    "## data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a8b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gps = pd.read_csv(\"data/processed_trips.csv\")\n",
    "\n",
    "df_people = pd.read_csv(\"data/individuals_dataset.csv\")\n",
    "df_people = df_people[df_people['GPS_RECORD'] == True]\n",
    "\n",
    "df_merged = pd.merge(\n",
    "    df_gps,\n",
    "    df_people[['ID', 'WEIGHT_INDIV']],\n",
    "    left_on='ID',\n",
    "    right_on='ID',\n",
    "    how='inner'\n",
    ")#.drop(columns='ID')\n",
    "df_merged = df_merged.rename(columns={\"ori_lat\": \"start_lat\", \"ori_lon\": \"start_lon\", \"dst_lat\":'end_lat',\"dst_lon\":'end_lon'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4a15b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_resolution = 10\n",
    "\n",
    "df_merged['start_h3'] = df_merged.apply(lambda row: h3.latlng_to_cell(row['start_lat'], row['start_lon'], h3_resolution), axis=1)\n",
    "df_merged['end_h3'] = df_merged.apply(lambda row: h3.latlng_to_cell(row['end_lat'], row['end_lon'], h3_resolution), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3bff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_weight = df_merged['WEIGHT_INDIV'].mean()\n",
    "avg_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f1487",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e64f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "od_matrix_first = df_merged.groupby(['start_h3', 'end_h3']).agg({\n",
    "    'WEIGHT_INDIV': ['sum', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "od_matrix_first.columns = ['start_h3', 'end_h3', 'total_weight', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af35e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singolo esagono res=5 : \"851fb467fffffff\"\n",
    "# 4 figli di res5 che coprono Parigi centro res=6 : \"861fb4667ffffff\", \"861fb4677ffffff\", \"861fb466fffffff\", \"861fb4647ffffff\"\n",
    "# res=7 \"871fb4674ffffff\", \"871fb475bffffff\", \"871fb4675ffffff\", \"871fb4666ffffff\", \"871fb4662ffffff\", \"871fb4660ffffff\"\n",
    "# \"861fb4667ffffff\", \"861fb4677ffffff\", \"861fb466fffffff\", \"861fb4647ffffff\", \"861fb4297ffffff\", \"861fb474fffffff\", \"861fb475fffffff\", \"861fb462fffffff\"\n",
    "parent_hexes = [\"861fb4667ffffff\", \"861fb4677ffffff\", \"861fb466fffffff\", \"861fb4647ffffff\", \"861fb475fffffff\"]\n",
    "\n",
    "# Genera la lista dei figli a risoluzione 10\n",
    "target_resolution = 10\n",
    "start_valid_h3 = set()\n",
    "end_valid_h3 = set()\n",
    "\n",
    "for parent in parent_hexes:\n",
    "    children = h3.cell_to_children(parent, target_resolution)  # <-- NON serve compact\n",
    "    for child in children:\n",
    "        start_valid_h3.add(child)\n",
    "        end_valid_h3.add(child)\n",
    "\n",
    "mask = (\n",
    "    (od_matrix_first[\"start_h3\"].isin(start_valid_h3))\n",
    "    & (od_matrix_first[\"end_h3\"].isin(end_valid_h3))\n",
    ")\n",
    "\n",
    "od_matrix_first = od_matrix_first[mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06116197",
   "metadata": {},
   "outputs": [],
   "source": [
    "od_matrix = od_matrix_first.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65958087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singolo esagono res=5 : \"851fb467fffffff\"\n",
    "# 4 figli di res5 che coprono Parigi centro res=6 : \"861fb4667ffffff\", \"861fb4677ffffff\", \"861fb466fffffff\", \"861fb4647ffffff\"\n",
    "# res=7 \"871fb4674ffffff\", \"871fb475bffffff\", \"871fb4675ffffff\", \"871fb4666ffffff\", \"871fb4662ffffff\", \"871fb4660ffffff\"\n",
    "# \"861fb4667ffffff\", \"861fb4677ffffff\", \"861fb466fffffff\", \"861fb4647ffffff\", \"861fb4297ffffff\", \"861fb474fffffff\", \"861fb475fffffff\", \"861fb462fffffff\"\n",
    "parent_hexes = [\"861fb4667ffffff\", \"861fb4677ffffff\", \"861fb466fffffff\", \"861fb4647ffffff\", \"861fb475fffffff\"]\n",
    "\n",
    "# Genera la lista dei figli a risoluzione 10\n",
    "target_resolution = 10\n",
    "start_valid_h3 = set()\n",
    "end_valid_h3 = set()\n",
    "\n",
    "for parent in parent_hexes:\n",
    "    children = h3.cell_to_children(parent, target_resolution)  # <-- NON serve compact\n",
    "    for child in children:\n",
    "        start_valid_h3.add(child)\n",
    "        end_valid_h3.add(child)\n",
    "\n",
    "mask = (\n",
    "    (od_matrix_first[\"start_h3\"].isin(start_valid_h3))\n",
    "    & (od_matrix_first[\"end_h3\"].isin(end_valid_h3))\n",
    ")\n",
    "\n",
    "od_matrix_first = od_matrix_first[mask].copy()\n",
    "print(f\"Numero di righe filtrate: {len(od_matrix_first):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a397e138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bc0c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "od_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57b210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_pre_generalization_filter(\n",
    "    od_matrix: pd.DataFrame,\n",
    "    k_threshold: int = 10,\n",
    "    max_generalization_levels: int = 3,\n",
    "    suppression_budget_percent: float = 0.1\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter out OD pairs that cannot become k-anonymous even when generalized to the maximum level.\n",
    "    \"\"\"\n",
    "    \n",
    "    original_size = len(od_matrix)\n",
    "    max_suppressions = int(original_size * suppression_budget_percent)\n",
    "\n",
    "    # Cache for H3 mappings at higher levels\n",
    "    mapping_cache = {}\n",
    "\n",
    "    def generalize(h, level_down):\n",
    "        res = h3.get_resolution(h)\n",
    "        target_res = res - level_down\n",
    "        if target_res < 0:\n",
    "            target_res = 0\n",
    "        key = (h, target_res)\n",
    "        if key not in mapping_cache:\n",
    "            mapping_cache[key] = h3.cell_to_parent(h, target_res) if res > target_res else h\n",
    "        return mapping_cache[key]\n",
    "\n",
    "    od_working = od_matrix.copy()\n",
    "    for lvl in range(max_generalization_levels + 1):\n",
    "        # Generalize start and end up to level `lvl`:\n",
    "        # Starting from the original hexagons (res=10), find the parents for each level up to `max_generalization_levels`.\n",
    "        # So given a hexagon of res=10, the `generalize` function will return the parent at resolution 9, 8, 7.\n",
    "        # If max_generalization_levels = 2: columns created: start_gen_0, end_gen_0, start_gen_1, end_gen_1, start_gen_2, end_gen_2 \n",
    "        # In each column, there will be the parent of the original hexagon at that level of generalization\n",
    "        od_working[f'start_gen_{lvl}'] = od_working['start_h3'].apply(lambda x: generalize(x, lvl))\n",
    "        od_working[f'end_gen_{lvl}'] = od_working['end_h3'].apply(lambda x: generalize(x, lvl))\n",
    "\n",
    "    results = []\n",
    "    for lvl in range(max_generalization_levels + 1):\n",
    "        # Group by levels of generalization and count occurrences\n",
    "        grouped = od_working.groupby([f'start_gen_{lvl}', f'end_gen_{lvl}'])['count'].sum().reset_index()\n",
    "        grouped.columns = ['start_gen', 'end_gen', 'agg_count']\n",
    "        grouped['level'] = lvl\n",
    "        results.append(grouped)\n",
    "\n",
    "    all_levels = pd.concat(results)\n",
    "    \n",
    "    od_with_id = od_working.reset_index().rename(columns={'index': 'row_id'})\n",
    "\n",
    "    \n",
    "    valid_pairs = set()\n",
    "    for lvl in range(max_generalization_levels + 1):\n",
    "        merged = od_with_id.merge(\n",
    "            all_levels[all_levels['level'] == lvl],\n",
    "            left_on=[f'start_gen_{lvl}', f'end_gen_{lvl}'],\n",
    "            right_on=['start_gen', 'end_gen'],\n",
    "            how='left'\n",
    "        )\n",
    "        # For each level of generalization, check the agg_count, which is the sum of the counts from the child up to the parent at that level.\n",
    "        # If the agg_count is greater than or equal to k_threshold, then the pair is valid.\n",
    "        # There will be pairs for which, even with the agg_count relative to the largest parent, they will not reach k => this means that they can never be k-anonymous within that level.\n",
    "        # Add all rows that are k-anonymous at this level.\n",
    "        valid = merged[merged['agg_count'] >= k_threshold]\n",
    "        valid_pairs.update(valid['row_id'].tolist())\n",
    "\n",
    "    # identify rows that cannot be anonymized after generalization\n",
    "    all_row_ids = set(od_with_id['row_id'].tolist())\n",
    "    problematic_rows = all_row_ids - valid_pairs\n",
    "    \n",
    "    print(f\"k-anonymized rows {len(valid_pairs)}\")\n",
    "    print(f\"rows cannot be anonymized: {len(problematic_rows)}\")\n",
    "\n",
    "    # for rows cannot be anonymized\n",
    "    if len(problematic_rows) <= max_suppressions:\n",
    "        # removes all if budget is sufficient: \n",
    "        \n",
    "        rows_to_keep = valid_pairs\n",
    "        suppressed_count = len(problematic_rows)\n",
    "    else:\n",
    "        # choose which to remove when budget not sufficient\n",
    "        \n",
    "        problematic_df = od_with_id[od_with_id['row_id'].isin(problematic_rows)].copy()\n",
    "        \n",
    "        # First delete the rows with the lowest count\n",
    "        # nsmallest : return the first n rows with the smallest values in columns\n",
    "        to_suppress = problematic_df.nsmallest(max_suppressions, 'count')['row_id'].tolist()\n",
    "        \n",
    "        # Keep k-anonymous lines + problematic lines not suppressed\n",
    "        rows_to_suppress = set(to_suppress)\n",
    "        rows_to_keep = valid_pairs | (problematic_rows - rows_to_suppress)\n",
    "        suppressed_count = len(to_suppress)\n",
    "        \n",
    "        \n",
    "\n",
    "    # Return only valid rows with the columns start_h3, end_h3, and count.\n",
    "    filtered = od_with_id[od_with_id['row_id'].isin(rows_to_keep)].copy()\n",
    "    filtered = filtered[['start_h3', 'end_h3', 'total_weight', 'count']]\n",
    "\n",
    "    kept_count = len(filtered)\n",
    "    \n",
    "    print(f\"Lines retained: {kept_count} / {original_size} ({kept_count/original_size*100:.1f}%)\")\n",
    "    print(f\"Suppressed lines: {suppressed_count} ({suppressed_count/original_size*100:.1f}%)\")\n",
    "    print(f\"Budget used: {suppressed_count} / {max_suppressions}\")\n",
    "\n",
    "    return filtered, suppressed_count\n",
    "\n",
    "od_matrix, suppressed_count = fast_pre_generalization_filter(od_matrix, k_threshold=10, max_generalization_levels=3, suppression_budget_percent=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9f1e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppressed_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fc459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_merged.merge(\n",
    "    od_matrix[['start_h3', 'end_h3']],\n",
    "    on=['start_h3', 'end_h3'],\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d92abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a636e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "od_sorted = od_matrix.sort_values(by='count', ascending=False)\n",
    "top_n = 50\n",
    "top_od = od_sorted.head(top_n).copy()\n",
    "\n",
    "\n",
    "top_od['route'] = top_od['start_h3'] + ' â†’ ' + top_od['end_h3']\n",
    "\n",
    "# # Plot\n",
    "# plt.figure(figsize=(14, 6))\n",
    "# ax = sns.barplot(data=top_od, x='route', y='count', hue='route', palette='viridis', legend=False)\n",
    "\n",
    "# plt.xticks(rotation=45, ha='right')\n",
    "# plt.xlabel(\"Tratta OD (H3)\")\n",
    "# plt.ylabel(\"Conteggio\")\n",
    "# plt.title(f\"Top {top_n} tratte origine-destinazione (H3)\")\n",
    "\n",
    "# # Etichette dei conteggi\n",
    "# for container in ax.containers:\n",
    "#     ax.bar_label(container, fmt='%d', label_type='edge', padding=2, fontsize=8)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79342e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_below_10 = od_matrix[od_matrix['count'] < 10].shape[0]\n",
    "num_above_10 = od_matrix[od_matrix['count'] > 10].shape[0]\n",
    "num_equal_10 = od_matrix[od_matrix['count'] == 10].shape[0]\n",
    "\n",
    "print(f\"Number of pairs with count = 10: {num_equal_10}\")\n",
    "print(f\"Number of pairs with count < 10: {num_below_10}\")\n",
    "print(f\"Number of pairs with count > 10: {num_above_10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013f5ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class H3TreeNode:\n",
    "    \"\"\"H3 hierarchy tree node\"\"\"\n",
    "    def __init__(self, h3_id: str, resolution: int, total_weight: int = 0, count: int = 0):\n",
    "        self.h3_id = h3_id\n",
    "        self.resolution = resolution\n",
    "        self.total_weight = total_weight\n",
    "        self.count = count\n",
    "        self.children: Dict[str, 'H3TreeNode'] = {}\n",
    "        self.parent: Optional['H3TreeNode'] = None\n",
    "    \n",
    "    def add_child(self, child: 'H3TreeNode'):\n",
    "        \"\"\"Adds a child to the node\"\"\"\n",
    "        self.children[child.h3_id] = child\n",
    "        child.parent = self\n",
    "    \n",
    "    def add_weight(self, weight: int):\n",
    "        self.total_weight += weight\n",
    "        if self.parent:\n",
    "            self.parent.add_weight(weight)\n",
    "    \n",
    "    def add_count(self, count: int):\n",
    "        self.count += count\n",
    "        if self.parent:\n",
    "            self.parent.add_count(count)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (f\"H3Node(id={self.h3_id}, res={self.resolution}, \"\n",
    "                f\"total_weight={self.total_weight}, count={self.count}, children={len(self.children)})\")\n",
    "\n",
    "class H3HierarchicalTree:\n",
    "    def __init__(self, od_matrix: pd.DataFrame, target_resolution: int = 11, hex_column: str = 'start_h3'):\n",
    "        self.od_matrix = od_matrix\n",
    "        self.target_resolution = target_resolution\n",
    "        self.hex_column = hex_column  # 'start_h3' o 'end_h3'\n",
    "        self.nodes: Dict[str, H3TreeNode] = {}\n",
    "        self.root = None\n",
    "        self.min_resolution = None  # SarÃ  calcolato dinamicamente\n",
    "        \n",
    "    def get_all_hexagons(self) -> Set[str]:\n",
    "        \"\"\"Extracts all unique hexagons from the specified column\"\"\"\n",
    "        return set(self.od_matrix[self.hex_column].unique())\n",
    "    \n",
    "    def get_resolution_coverage(self, hexagons: Set[str], target_res: int) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Obtains all target resolution hexagons covering the area defined by the input hexagons.\n",
    "        \"\"\"\n",
    "        coverage_hexagons = set()\n",
    "        \n",
    "        for hex_id in hexagons:\n",
    "            current_res = h3.get_resolution(hex_id)\n",
    "            \n",
    "            if current_res == target_res:\n",
    "                coverage_hexagons.add(hex_id)\n",
    "            elif current_res < target_res:\n",
    "                # Dobbiamo espandere a risoluzione piÃ¹ alta\n",
    "                children = self._get_all_children_at_resolution(hex_id, target_res)\n",
    "                coverage_hexagons.update(children)\n",
    "            else:\n",
    "                # Dobbiamo salire a risoluzione piÃ¹ bassa\n",
    "                parent = h3.cell_to_parent(hex_id, target_res)\n",
    "                coverage_hexagons.add(parent)\n",
    "        \n",
    "        return coverage_hexagons\n",
    "    \n",
    "    def find_optimal_min_resolution(self, hexagons: Set[str]) -> int:\n",
    "        \"\"\"\n",
    "        Find the highest resolution where there is still only one node covering all hexagons.\n",
    "        \"\"\"        \n",
    "        # For each resolution from 0 to the target, count how many nodes are needed to cover all hexagons.\n",
    "        resolution_stats = {}\n",
    "        \n",
    "        for resolution in range(0, self.target_resolution + 1):\n",
    "            ancestors = set()\n",
    "            for hex_id in hexagons:\n",
    "                current_res = h3.get_resolution(hex_id)\n",
    "                if current_res >= resolution:\n",
    "                    ancestor = h3.cell_to_parent(hex_id, resolution)\n",
    "                    ancestors.add(ancestor)\n",
    "                else:\n",
    "                    # add the hexagon directly if already with a lower resolution, \n",
    "                    ancestors.add(hex_id)\n",
    "            \n",
    "            resolution_stats[resolution] = len(ancestors)\n",
    "            print(f\"Resolution {resolution}: {len(ancestors)} nodes\")\n",
    "        \n",
    "        # Find the highest resolution with count = 1\n",
    "        optimal_resolution = 0\n",
    "        for resolution in range(self.target_resolution, -1, -1):\n",
    "            if resolution_stats[resolution] == 1:\n",
    "                optimal_resolution = resolution\n",
    "                break\n",
    "        \n",
    "        # print(f\"optimal resolution: {optimal_resolution}\")\n",
    "        return optimal_resolution\n",
    "    \n",
    "    def get_siblings(self, node_id: str) -> List[str]:\n",
    "        \"\"\"Returns the h3_ids of the sibling nodes of node_id (other hexagons within the same parent)\"\"\"\n",
    "        if node_id not in self.nodes:\n",
    "            return []\n",
    "\n",
    "        node = self.nodes[node_id]\n",
    "        parent = node.parent\n",
    "        if parent is None:\n",
    "            return []\n",
    "\n",
    "        siblings = [child.h3_id for child in parent.children.values() if child.h3_id != node_id]\n",
    "        return siblings\n",
    "    \n",
    "    def get_parent(self, node_id: str) -> Optional[str]:\n",
    "        \"\"\"Returns the h3_id of the parent node, or None for root node.\"\"\"\n",
    "        if node_id not in self.nodes:\n",
    "            return None\n",
    "        node = self.nodes[node_id]\n",
    "        if node.parent is None:\n",
    "            return None\n",
    "        return node.parent.h3_id\n",
    "    \n",
    "    def _get_all_children_at_resolution(self, hex_id: str, target_res: int) -> Set[str]:\n",
    "        \"\"\"Recursively obtains all children at a specific resolution\"\"\"\n",
    "        current_res = h3.get_resolution(hex_id)\n",
    "        \n",
    "        if current_res == target_res:\n",
    "            return {hex_id}\n",
    "        elif current_res > target_res:\n",
    "            return set()\n",
    "        \n",
    "        children = set()\n",
    "        direct_children = h3.cell_to_children(hex_id, current_res + 1)\n",
    "        \n",
    "        for child in direct_children:\n",
    "            children.update(self._get_all_children_at_resolution(child, target_res))\n",
    "        \n",
    "        return children\n",
    "    \n",
    "    def build_hierarchy_path(self, hex_id: str, min_resolution: int) -> List[str]:\n",
    "        \"\"\"Builds the hierarchical path from a hexagon to the minimum resolution\"\"\"\n",
    "        path = [hex_id]\n",
    "        current = hex_id\n",
    "        current_res = h3.get_resolution(current)\n",
    "        \n",
    "        while current_res > min_resolution:\n",
    "            parent = h3.cell_to_parent(current, current_res - 1)\n",
    "            path.append(parent)\n",
    "            current = parent\n",
    "            current_res -= 1\n",
    "        \n",
    "        return path\n",
    "    \n",
    "    def create_tree_structure(self):\n",
    "        \"\"\"Create the optimized tree structure\"\"\"\n",
    "        target_hexagons = self.get_all_hexagons()\n",
    "        \n",
    "        # Get full coverage at the target resolution\n",
    "        coverage_hexagons = self.get_resolution_coverage(target_hexagons, self.target_resolution)\n",
    "        \n",
    "        # Find the optimal minimum resolution (the highest one with count=1)\n",
    "        self.min_resolution = self.find_optimal_min_resolution(coverage_hexagons)\n",
    "        \n",
    "        # print(f\"resolution tree from {self.min_resolution} to {self.target_resolution}\")\n",
    "        \n",
    "        # Build all hierarchical paths\n",
    "        all_paths = []\n",
    "        for hex_id in coverage_hexagons:\n",
    "            path = self.build_hierarchy_path(hex_id, self.min_resolution)\n",
    "            all_paths.append(path)\n",
    "        \n",
    "        for path in all_paths:\n",
    "            for hex_id in path:\n",
    "                if hex_id not in self.nodes:\n",
    "                    resolution = h3.get_resolution(hex_id)\n",
    "                    self.nodes[hex_id] = H3TreeNode(hex_id, resolution)\n",
    "        \n",
    "        # parent-child relation\n",
    "        for path in all_paths:\n",
    "            for i in range(len(path) - 1):\n",
    "                child_id = path[i]\n",
    "                parent_id = path[i + 1]\n",
    "                self.nodes[parent_id].add_child(self.nodes[child_id])\n",
    "        \n",
    "        # Identify the root\n",
    "        self.root = self.nodes[self._find_root_hexagon(coverage_hexagons, self.min_resolution)]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _find_root_hexagon(self, hexagons: Set[str], min_resolution: int) -> str:\n",
    "        sample_hex = next(iter(hexagons))\n",
    "        return h3.cell_to_parent(sample_hex, min_resolution)\n",
    "    \n",
    "    def populate_counts(self):\n",
    "        # Group the two metrics by hexagon\n",
    "        agg_df = self.od_matrix.groupby(self.hex_column).agg({'total_weight': 'sum', 'count': 'sum'}).reset_index()\n",
    "        \n",
    "        for _, row in agg_df.iterrows():\n",
    "            hex_id = row[self.hex_column]\n",
    "            weight = int(row['total_weight'])\n",
    "            count = int(row['count'])\n",
    "            \n",
    "            target_hex = self._map_to_target_resolution(hex_id)\n",
    "            \n",
    "            if target_hex in self.nodes:\n",
    "                self.nodes[target_hex].add_weight(weight)\n",
    "                self.nodes[target_hex].add_count(count)\n",
    "            else:\n",
    "                print(f\"hexagon {target_hex} not found\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _map_to_target_resolution(self, hex_id: str) -> str:\n",
    "        \"\"\"Map a hexagon to the target resolution\"\"\"\n",
    "        current_res = h3.get_resolution(hex_id)\n",
    "        \n",
    "        if current_res == self.target_resolution:\n",
    "            return hex_id\n",
    "        elif current_res < self.target_resolution:\n",
    "            # Take the first child available\n",
    "            children = self._get_all_children_at_resolution(hex_id, self.target_resolution)\n",
    "            return next(iter(children)) if children else hex_id\n",
    "        else:\n",
    "            return h3.cell_to_parent(hex_id, self.target_resolution)\n",
    "    \n",
    "    def get_tree_statistics(self) -> Dict:\n",
    "        if not self.root:\n",
    "            return {}\n",
    "        \n",
    "        stats = {\n",
    "            'total_nodes': len(self.nodes),\n",
    "            'root_resolution': self.root.resolution,\n",
    "            'min_resolution': self.min_resolution,\n",
    "            'target_resolution': self.target_resolution,\n",
    "            'total_weight': self.root.total_weight,\n",
    "            'nodes_by_resolution': defaultdict(int),\n",
    "            'resolution_range': f\"{self.min_resolution} â†’ {self.target_resolution}\"\n",
    "        }\n",
    "        \n",
    "        for node in self.nodes.values():\n",
    "            stats['nodes_by_resolution'][node.resolution] += 1\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def print_tree(self, max_children_per_level: int = 10):\n",
    "        if not self.root:\n",
    "            print(\"Tree not built\")\n",
    "            return\n",
    "        \n",
    "        print(f\"OPTIMIZED TREE STRUCTURE: resolution range {self.min_resolution} â†’ {self.target_resolution}\")\n",
    "        \n",
    "        def _print_node(node: H3TreeNode, depth: int = 0, is_last: bool = True, prefix: str = \"\"):\n",
    "            connector = \"â””â”€ \" if is_last else \"â”œâ”€ \"\n",
    "            print(f\"{prefix}{connector}{node.h3_id} (res:{node.resolution}, total_weight:{node.total_weight}, count:{node.count}, children:{len(node.children)})\")\n",
    "            \n",
    "            if is_last:\n",
    "                child_prefix = prefix + \"   \"\n",
    "            else:\n",
    "                child_prefix = prefix + \"â”‚  \"\n",
    "            \n",
    "            children_list = list(node.children.values())\n",
    "            \n",
    "            if len(children_list) <= max_children_per_level:\n",
    "                for i, child in enumerate(children_list):\n",
    "                    is_last_child = (i == len(children_list) - 1)\n",
    "                    _print_node(child, depth + 1, is_last_child, child_prefix)\n",
    "            else:\n",
    "                for i in range(max_children_per_level):\n",
    "                    child = children_list[i]\n",
    "                    is_last_child = (i == max_children_per_level - 1) and (len(children_list) == max_children_per_level)\n",
    "                    _print_node(child, depth + 1, is_last_child, child_prefix)\n",
    "                \n",
    "                remaining = len(children_list) - max_children_per_level\n",
    "                print(f\"{child_prefix}â””â”€ ... and other {remaining} children with the same pattern\")\n",
    "        \n",
    "        _print_node(self.root, 0, True, \"\")\n",
    "\n",
    "def create_h3_hierarchical_tree(od_matrix_df: pd.DataFrame, target_resolution: int = 10, hex_column: str = 'start_h3'):\n",
    "    \"\"\"\n",
    "    Create an optimized H3 hierarchical tree from the OD matrix dataset\n",
    "    \n",
    "    Args:\n",
    "        od_matrix_df: DataFrame with column 'start_h3', 'end_h3', 'count'\n",
    "        target_resolution: Target resolution for tree leaves\n",
    "        hex_column: Column to be analyzed ('start_h3' o 'end_h3')\n",
    "    \n",
    "    Returns:\n",
    "        H3HierarchicalTree: Constructed and optimized hierarchical tree\n",
    "    \"\"\"\n",
    "    \n",
    "    tree = H3HierarchicalTree(od_matrix_df, target_resolution, hex_column)\n",
    "    tree.create_tree_structure()\n",
    "    tree.populate_counts()\n",
    "    \n",
    "    stats = tree.get_tree_statistics()\n",
    "    print(f\"OPTIMIZED TREE STATISTICS ({hex_column.upper()})\")\n",
    "    print(stats)\n",
    "    \n",
    "    # Calculate savings in nodes\n",
    "    total_resolutions_possible = target_resolution + 1  # da 0 a target\n",
    "    resolutions_used = len(stats['nodes_by_resolution'])\n",
    "    resolutions_saved = total_resolutions_possible - resolutions_used\n",
    "    \n",
    "    print(f\"OPTIMIZATIONS:\")\n",
    "    print(f\"Saved resolutions: {resolutions_saved}\")\n",
    "    print(f\"Tree efficiency: {resolutions_used}/{total_resolutions_possible} livelli utilizzati\")\n",
    "    \n",
    "    tree.print_tree()\n",
    "    \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c16653",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_start = create_h3_hierarchical_tree(od_matrix, target_resolution=10, hex_column='start_h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86051b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_end = create_h3_hierarchical_tree(od_matrix, target_resolution=10, hex_column='end_h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186e1c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium import Map, Polygon\n",
    "from matplotlib import cm, colors\n",
    "\n",
    "def plot_h3_hexagons(hex_counts: dict, map_center=None, color_map='YlOrRd', zoom_start=11):\n",
    "    \"\"\"\n",
    "    Plot H3 on map with Folium.\n",
    "\n",
    "    Args:\n",
    "        hex_counts: dictionary {h3_id: count}\n",
    "        map_center: coordinates [lat, lng] for the map center (optional)\n",
    "        color_map: matplotlib colormap name\n",
    "        zoom_start: initial zoom level\n",
    "    \"\"\"\n",
    "    if not hex_counts:\n",
    "        print(\"No hexagons to show\")\n",
    "        return None\n",
    "\n",
    "    # Calcolo centro mappa se non specificato\n",
    "    if not map_center:\n",
    "        lat_lngs = [h3.cell_to_latlng(h) for h in hex_counts]\n",
    "        avg_lat = sum(lat for lat, _ in lat_lngs) / len(lat_lngs)\n",
    "        avg_lng = sum(lng for _, lng in lat_lngs) / len(lat_lngs)\n",
    "        map_center = [avg_lat, avg_lng]\n",
    "\n",
    "    fmap = Map(location=map_center, zoom_start=zoom_start, tiles='cartodbpositron')\n",
    "\n",
    "    max_count = max(hex_counts.values())\n",
    "    norm = colors.Normalize(vmin=0, vmax=max_count)\n",
    "    cmap = cm.get_cmap(color_map)\n",
    "\n",
    "    for h3_id, count in hex_counts.items():\n",
    "        boundary = h3.cell_to_boundary(h3_id)\n",
    "        color = colors.to_hex(cmap(norm(count)))\n",
    "        Polygon(locations=boundary, color=color, fill=True, fill_opacity=0.6).add_to(fmap)\n",
    "\n",
    "    return fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b509056",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = {\n",
    "    node.h3_id: node.count\n",
    "   for node in tree_start.nodes.values() }\n",
    "\n",
    "plot_h3_hexagons(all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dabc3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, Set, List, Optional, Tuple\n",
    "import scipy.sparse as sp\n",
    "from numba import jit, njit\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h3\n",
    "\n",
    "class OptimizedH3GeneralizedODMatrix:\n",
    "    \"\"\"\n",
    "    Version optimized for very large OD matrices with count-based k-anonymity\n",
    "    Main matrix: counts (for k-anonymity)\n",
    "    Secondary matrix: weights (stored for analysis)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, od_matrix: pd.DataFrame, tree_start, tree_end, k_threshold: int = 10):\n",
    "        self.original_od_matrix = od_matrix\n",
    "        self.tree_start = tree_start\n",
    "        self.tree_end = tree_end\n",
    "        self.k_threshold = k_threshold\n",
    "        \n",
    "        # Usa matrici sparse separate per count (principale) e pesi (secondaria)\n",
    "        self.current_matrix_sparse = None      # Matrice dei COUNT (per k-anonimity)\n",
    "        self.current_weights_sparse = None     # Matrice dei PESI (secondaria)\n",
    "        \n",
    "        # Mappature ottimizzate\n",
    "        self.start_to_idx = {}  # {h3_id: index}\n",
    "        self.end_to_idx = {}    # {h3_id: index}\n",
    "        self.idx_to_start = {}  # {index: h3_id}\n",
    "        self.idx_to_end = {}    # {index: h3_id}\n",
    "        \n",
    "        # Cache per evitare ricalcoli\n",
    "        self.sibling_groups_cache = {}\n",
    "        self.parent_cache = {}\n",
    "        \n",
    "        # Storia delle generalizzazioni\n",
    "        self.generalization_history = []\n",
    "        \n",
    "    def initialize_optimized_matrix(self):\n",
    "    \n",
    "        # pre-filter the data: remove cells with count = 0\n",
    "        non_zero_od = self.original_od_matrix[self.original_od_matrix['count'] > 0].copy()\n",
    "        \n",
    "        # 2. Ottieni solo gli esagoni effettivamente utilizzati\n",
    "        used_starts = set(non_zero_od['start_h3'].unique())\n",
    "        used_ends = set(non_zero_od['end_h3'].unique())\n",
    "        \n",
    "        # 3. Mappa a risoluzione target solo gli esagoni utilizzati\n",
    "        target_starts = self._get_target_resolution_hexagons(used_starts, self.tree_start)\n",
    "        target_ends = self._get_target_resolution_hexagons(used_ends, self.tree_end)\n",
    "        \n",
    "        print(f\"ðŸŽ¯ Esagoni target: {len(target_starts)} origini, {len(target_ends)} destinazioni\")\n",
    "        \n",
    "        # 4. Crea mappature indicizzate\n",
    "        self.start_to_idx = {h3_id: idx for idx, h3_id in enumerate(sorted(target_starts))}\n",
    "        self.end_to_idx = {h3_id: idx for idx, h3_id in enumerate(sorted(target_ends))}\n",
    "        self.idx_to_start = {idx: h3_id for h3_id, idx in self.start_to_idx.items()}\n",
    "        self.idx_to_end = {idx: h3_id for h3_id, idx in self.end_to_idx.items()}\n",
    "        \n",
    "        # 5. Costruisci matrici sparse separate per count (principale) e pesi (secondaria)\n",
    "        self.current_matrix_sparse, self.current_weights_sparse = self._build_sparse_matrices(\n",
    "            non_zero_od, target_starts, target_ends\n",
    "        )\n",
    "        \n",
    "        # 6. Pre-calcola i gruppi di siblings\n",
    "        self._precompute_sibling_groups()\n",
    "        \n",
    "        print(f\"âœ… Matrici sparse inizializzate: {self.current_matrix_sparse.shape} \"\n",
    "              f\"({self.current_matrix_sparse.nnz:,} elementi non-zero)\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _get_target_resolution_hexagons(self, hexagons: Set[str], tree) -> Set[str]:\n",
    "        \"\"\"Ottiene esagoni target risoluzione solo per quelli utilizzati\"\"\"\n",
    "        target_hexagons = set()\n",
    "        target_res = tree.target_resolution\n",
    "        \n",
    "        for hex_id in hexagons:\n",
    "            current_res = h3.get_resolution(hex_id)\n",
    "            \n",
    "            if current_res == target_res:\n",
    "                target_hexagons.add(hex_id)\n",
    "            elif current_res < target_res:\n",
    "                # Espandi solo se Ã¨ nei nodi dell'albero\n",
    "                if hex_id in tree.nodes:\n",
    "                    children = self._get_children_at_resolution_fast(hex_id, target_res)\n",
    "                    target_hexagons.update(children)\n",
    "            else:\n",
    "                parent = h3.cell_to_parent(hex_id, target_res)\n",
    "                target_hexagons.add(parent)\n",
    "        \n",
    "        return target_hexagons\n",
    "    \n",
    "    def _get_children_at_resolution_fast(self, hex_id: str, target_res: int) -> Set[str]:\n",
    "        \"\"\"Versione veloce per ottenere figli\"\"\"\n",
    "        current_res = h3.get_resolution(hex_id)\n",
    "        if current_res == target_res:\n",
    "            return {hex_id}\n",
    "        \n",
    "        # Usa cache se disponibile\n",
    "        cache_key = (hex_id, target_res)\n",
    "        if cache_key in self.sibling_groups_cache:\n",
    "            return self.sibling_groups_cache[cache_key]\n",
    "        \n",
    "        children = set()\n",
    "        queue = [hex_id]\n",
    "        \n",
    "        while queue:\n",
    "            current = queue.pop(0)\n",
    "            current_r = h3.get_resolution(current)\n",
    "            \n",
    "            if current_r == target_res:\n",
    "                children.add(current)\n",
    "            elif current_r < target_res:\n",
    "                direct_children = h3.cell_to_children(current, current_r + 1)\n",
    "                queue.extend(direct_children)\n",
    "        \n",
    "        self.sibling_groups_cache[cache_key] = children\n",
    "        return children\n",
    "    \n",
    "    def _build_sparse_matrices(self, od_data: pd.DataFrame, target_starts: Set[str], target_ends: Set[str]) -> Tuple[sp.csr_matrix, sp.csr_matrix]:\n",
    "        \"\"\"Costruisce matrici sparse separate per count (principale) e pesi (secondaria)\"\"\"\n",
    "        rows_counts, cols_counts, data_counts = [], [], []\n",
    "        rows_weights, cols_weights, data_weights = [], [], []\n",
    "        \n",
    "        print(\"ðŸ”¨ Costruzione matrici sparse...\")\n",
    "        \n",
    "        # Raggruppa i dati per efficienza - somma sia count che pesi\n",
    "        grouped = od_data.groupby(['start_h3', 'end_h3']).agg({\n",
    "            'count': 'sum',\n",
    "            'total_weight': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        for _, row in grouped.iterrows():\n",
    "            start_h3 = row['start_h3']\n",
    "            end_h3 = row['end_h3']\n",
    "            count = row['count']\n",
    "            weight = row['total_weight']\n",
    "            \n",
    "            # Mappa agli esagoni target\n",
    "            mapped_start = self._map_to_target_fast(start_h3, target_starts, self.tree_start)\n",
    "            mapped_end = self._map_to_target_fast(end_h3, target_ends, self.tree_end)\n",
    "            \n",
    "            if mapped_start and mapped_end:\n",
    "                start_idx = self.start_to_idx[mapped_start]\n",
    "                end_idx = self.end_to_idx[mapped_end]\n",
    "                \n",
    "                # Aggiungi ai dati per entrambe le matrici\n",
    "                rows_counts.append(end_idx)\n",
    "                cols_counts.append(start_idx)\n",
    "                data_counts.append(count)\n",
    "                \n",
    "                rows_weights.append(end_idx)\n",
    "                cols_weights.append(start_idx)\n",
    "                data_weights.append(weight)\n",
    "        \n",
    "        # Crea matrici sparse COO e converti in CSR\n",
    "        shape = (len(target_ends), len(target_starts))\n",
    "        \n",
    "        # Matrice dei count (PRINCIPALE per k-anonimity)\n",
    "        matrix_counts_coo = sp.coo_matrix((data_counts, (rows_counts, cols_counts)), shape=shape)\n",
    "        matrix_counts_csr = matrix_counts_coo.tocsr()\n",
    "        matrix_counts_csr.sum_duplicates()\n",
    "        \n",
    "        # Matrice dei pesi (SECONDARIA)\n",
    "        matrix_weights_coo = sp.coo_matrix((data_weights, (rows_weights, cols_weights)), shape=shape)\n",
    "        matrix_weights_csr = matrix_weights_coo.tocsr()\n",
    "        matrix_weights_csr.sum_duplicates()\n",
    "        \n",
    "        return matrix_counts_csr, matrix_weights_csr\n",
    "    \n",
    "    def _map_to_target_fast(self, h3_id: str, target_nodes: Set[str], tree) -> Optional[str]:\n",
    "        \"\"\"Versione veloce di mapping con cache\"\"\"\n",
    "        if h3_id in target_nodes:\n",
    "            return h3_id\n",
    "        \n",
    "        # Usa cache\n",
    "        if h3_id in self.parent_cache:\n",
    "            cached_parent = self.parent_cache[h3_id]\n",
    "            if cached_parent in target_nodes:\n",
    "                return cached_parent\n",
    "        \n",
    "        # Calcola mapping\n",
    "        current_res = h3.get_resolution(h3_id)\n",
    "        target_res = tree.target_resolution\n",
    "        \n",
    "        if current_res > target_res:\n",
    "            parent = h3.cell_to_parent(h3_id, target_res)\n",
    "            self.parent_cache[h3_id] = parent\n",
    "            return parent if parent in target_nodes else None\n",
    "        elif current_res < target_res:\n",
    "            # Trova il primo target che contiene questo esagono\n",
    "            for target in target_nodes:\n",
    "                if self._is_descendant_fast(target, h3_id):\n",
    "                    return target\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _is_descendant_fast(self, child_h3: str, parent_h3: str) -> bool:\n",
    "        \"\"\"Versione ottimizzata di controllo discendenza\"\"\"\n",
    "        child_res = h3.get_resolution(child_h3)\n",
    "        parent_res = h3.get_resolution(parent_h3)\n",
    "        \n",
    "        if parent_res >= child_res:\n",
    "            return False\n",
    "        \n",
    "        # Cache check\n",
    "        cache_key = (child_h3, parent_h3)\n",
    "        if cache_key in self.parent_cache:\n",
    "            return self.parent_cache[cache_key]\n",
    "        \n",
    "        # Calcola\n",
    "        current = child_h3\n",
    "        while h3.get_resolution(current) > parent_res:\n",
    "            current = h3.cell_to_parent(current, h3.get_resolution(current) - 1)\n",
    "        \n",
    "        result = current == parent_h3\n",
    "        self.parent_cache[cache_key] = result\n",
    "        return result\n",
    "    \n",
    "    def _precompute_sibling_groups(self):\n",
    "        \"\"\"Pre-calcola tutti i gruppi di siblings possibili\"\"\"\n",
    "        print(\"ðŸ§  Pre-calcolo gruppi siblings...\")\n",
    "        \n",
    "        self.start_sibling_groups = self._compute_sibling_groups(self.start_to_idx.keys(), self.tree_start)\n",
    "        self.end_sibling_groups = self._compute_sibling_groups(self.end_to_idx.keys(), self.tree_end)\n",
    "        \n",
    "        print(f\"ðŸ“‹ Trovati {len(self.start_sibling_groups)} gruppi origine, {len(self.end_sibling_groups)} gruppi destinazione\")\n",
    "    \n",
    "    def _compute_sibling_groups(self, nodes: Set[str], tree) -> List[Tuple[List[str], str]]:\n",
    "        \"\"\"Calcola tutti i gruppi di siblings una volta sola - INCLUDE figli unici\"\"\"\n",
    "        groups = []\n",
    "        processed = set()\n",
    "        \n",
    "        for node_id in nodes:\n",
    "            if node_id in processed or node_id not in tree.nodes:\n",
    "                continue\n",
    "            \n",
    "            node = tree.nodes[node_id]\n",
    "            if not node.parent:\n",
    "                continue\n",
    "            \n",
    "            # Trova tutti i siblings (anche se Ã¨ uno solo)\n",
    "            siblings = []\n",
    "            for sibling_id in node.parent.children:\n",
    "                if sibling_id in nodes and sibling_id not in processed:\n",
    "                    siblings.append(sibling_id)\n",
    "            \n",
    "            if len(siblings) >= 1:  # Accetta anche figli unici\n",
    "                groups.append((siblings, node.parent.h3_id))\n",
    "                processed.update(siblings)\n",
    "        \n",
    "        return groups\n",
    "    \n",
    "    def get_best_generalization_fast(self, axis: str) -> Optional[Tuple[List[str], str, int]]:\n",
    "        \"\"\"Trova la migliore generalizzazione basata sui COUNT (k-anonimity)\"\"\"\n",
    "        if axis == 'columns':\n",
    "            tree = self.tree_start\n",
    "            mapping = self.start_to_idx\n",
    "            matrix = self.current_matrix_sparse.tocsc()  # Usa matrice dei COUNT\n",
    "        else:\n",
    "            tree = self.tree_end\n",
    "            mapping = self.end_to_idx\n",
    "            matrix = self.current_matrix_sparse.tocsr()  # Usa matrice dei COUNT\n",
    "\n",
    "        best_group, best_parent, best_cost = None, None, float('inf')\n",
    "        \n",
    "        for parent_id, parent_node in tree.nodes.items():\n",
    "            siblings = list(parent_node.children.keys())\n",
    "            \n",
    "            if len(siblings) < 1:  # Solo se non ha figli\n",
    "                continue\n",
    "                \n",
    "            # Filtra solo i siblings ancora presenti nella matrice\n",
    "            present = [s for s in siblings if s in mapping]\n",
    "            \n",
    "            # MODIFICA IMPORTANTE: non richiedere che TUTTI i siblings siano presenti\n",
    "            # per figli unici, basta che il figlio sia presente\n",
    "            if len(present) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Per figli unici: se c'Ã¨ 1 figlio presente e 1 figlio totale, OK\n",
    "            # Per piÃ¹ figli: richiedi che tutti siano presenti\n",
    "            if len(siblings) > 1 and len(present) != len(siblings):\n",
    "               continue\n",
    "\n",
    "            # Usa i count dal tree per il costo (basato sui COUNT)\n",
    "            cost = sum(tree.nodes[sibling_id].count for sibling_id in present)\n",
    "            \n",
    "            if cost < best_cost:\n",
    "                best_cost = cost\n",
    "                best_group = present  # Usa 'present' invece di 'siblings'\n",
    "                best_parent = parent_id\n",
    "\n",
    "        if best_group is None:\n",
    "            return None\n",
    "        return best_group, best_parent, best_cost\n",
    "    \n",
    "    def apply_sparse_generalization(self, group: List[str], parent_id: str, axis: str):\n",
    "        \"\"\"Applica generalizzazione su entrambe le matrici - funziona anche con gruppi di 1 elemento\"\"\"\n",
    "        if axis == 'columns':\n",
    "            indices = [self.start_to_idx[h3_id] for h3_id in group]\n",
    "\n",
    "            # Processa matrice dei count (PRINCIPALE)\n",
    "            counts_csc = self.current_matrix_sparse.tocsc()\n",
    "            combined_counts_col = counts_csc[:, indices].sum(axis=1).A1\n",
    "            \n",
    "            # Processa matrice dei pesi (SECONDARIA)\n",
    "            weights_csc = self.current_weights_sparse.tocsc()\n",
    "            combined_weights_col = weights_csc[:, indices].sum(axis=1).A1\n",
    "            \n",
    "            # Rimuovi colonne vecchie da entrambe le matrici\n",
    "            mask = np.ones(counts_csc.shape[1], dtype=bool)\n",
    "            mask[indices] = False\n",
    "            counts_reduced = counts_csc[:, mask]\n",
    "            weights_reduced = weights_csc[:, mask]\n",
    "\n",
    "            # Aggiungi nuove colonne\n",
    "            combined_counts_sparse = sp.csr_matrix(combined_counts_col).T\n",
    "            combined_weights_sparse = sp.csr_matrix(combined_weights_col).T\n",
    "            \n",
    "            self.current_matrix_sparse = sp.hstack([counts_reduced, combined_counts_sparse]).tocsr()\n",
    "            self.current_weights_sparse = sp.hstack([weights_reduced, combined_weights_sparse]).tocsr()\n",
    "\n",
    "            # Aggiorna mappature\n",
    "            new_start_to_idx = {}\n",
    "            new_idx_to_start = {}\n",
    "            new_idx = 0\n",
    "\n",
    "            for old_idx, h3_id in self.idx_to_start.items():\n",
    "                if h3_id not in group:\n",
    "                    new_start_to_idx[h3_id] = new_idx\n",
    "                    new_idx_to_start[new_idx] = h3_id\n",
    "                    new_idx += 1\n",
    "\n",
    "            new_start_to_idx[parent_id] = new_idx\n",
    "            new_idx_to_start[new_idx] = parent_id\n",
    "\n",
    "            self.start_to_idx = new_start_to_idx\n",
    "            self.idx_to_start = new_idx_to_start\n",
    "\n",
    "        else:\n",
    "            # Stesso per le righe\n",
    "            indices = [self.end_to_idx[h3_id] for h3_id in group]\n",
    "\n",
    "            # Processa matrice dei count (PRINCIPALE)\n",
    "            counts_csr = self.current_matrix_sparse.tocsr()\n",
    "            combined_counts_row = counts_csr[indices, :].sum(axis=0).A1\n",
    "\n",
    "            # Processa matrice dei pesi (SECONDARIA)\n",
    "            weights_csr = self.current_weights_sparse.tocsr()\n",
    "            combined_weights_row = weights_csr[indices, :].sum(axis=0).A1\n",
    "\n",
    "            # Rimuovi righe vecchie da entrambe le matrici\n",
    "            mask = np.ones(counts_csr.shape[0], dtype=bool)\n",
    "            mask[indices] = False\n",
    "            counts_reduced = counts_csr[mask, :]\n",
    "            weights_reduced = weights_csr[mask, :]\n",
    "\n",
    "            # Aggiungi nuove righe\n",
    "            combined_counts_sparse = sp.csr_matrix(combined_counts_row)\n",
    "            combined_weights_sparse = sp.csr_matrix(combined_weights_row)\n",
    "            \n",
    "            self.current_matrix_sparse = sp.vstack([counts_reduced, combined_counts_sparse]).tocsr()\n",
    "            self.current_weights_sparse = sp.vstack([weights_reduced, combined_weights_sparse]).tocsr()\n",
    "\n",
    "            # Aggiorna mappature\n",
    "            new_end_to_idx = {}\n",
    "            new_idx_to_end = {}\n",
    "            new_idx = 0\n",
    "\n",
    "            for old_idx, h3_id in self.idx_to_end.items():\n",
    "                if h3_id not in group:\n",
    "                    new_end_to_idx[h3_id] = new_idx\n",
    "                    new_idx_to_end[new_idx] = h3_id\n",
    "                    new_idx += 1\n",
    "\n",
    "            new_end_to_idx[parent_id] = new_idx\n",
    "            new_idx_to_end[new_idx] = parent_id\n",
    "\n",
    "            self.end_to_idx = new_end_to_idx\n",
    "            self.idx_to_end = new_idx_to_end\n",
    "\n",
    "        # Rimuovi i gruppi generalizzati (funziona anche per gruppi di 1)\n",
    "        if axis == 'columns':\n",
    "            self.start_sibling_groups = [\n",
    "                (sibs, par) for sibs, par in self.start_sibling_groups \n",
    "                if not any(s in group for s in sibs)\n",
    "            ]\n",
    "        else:\n",
    "            self.end_sibling_groups = [\n",
    "                (sibs, par) for sibs, par in self.end_sibling_groups \n",
    "                if not any(s in group for s in sibs)\n",
    "            ]\n",
    "\n",
    "        # Aggiorna la struttura dei siblings con il nuovo nodo\n",
    "        if axis == 'columns':\n",
    "            parent_node = self.tree_start.nodes.get(parent_id)\n",
    "            if parent_node and parent_node.parent:\n",
    "                siblings_left = [s for s in parent_node.parent.children.keys() \n",
    "                            if s in self.start_to_idx and s != parent_id]\n",
    "                if len(siblings_left) >= 1:\n",
    "                    grandparent_id = parent_node.parent.h3_id\n",
    "                    self.start_sibling_groups.append((siblings_left + [parent_id], grandparent_id))\n",
    "        else:\n",
    "            parent_node = self.tree_end.nodes.get(parent_id)\n",
    "            if parent_node and parent_node.parent:\n",
    "                siblings_left = [s for s in parent_node.parent.children.keys() \n",
    "                            if s in self.end_to_idx and s != parent_id]\n",
    "                if len(siblings_left) >= 1:\n",
    "                    grandparent_id = parent_node.parent.h3_id\n",
    "                    self.end_sibling_groups.append((siblings_left + [parent_id], grandparent_id))\n",
    "    \n",
    "    def get_min_value_sparse(self) -> int:\n",
    "        \"\"\"Ottiene valore minimo da matrice sparse dei COUNT (per k-anonimity)\"\"\"\n",
    "        if self.current_matrix_sparse.nnz == 0:\n",
    "            return 0\n",
    "        return self.current_matrix_sparse.data.min()\n",
    "    \n",
    "    def run_optimized_generalization(self) -> Tuple[sp.csr_matrix, sp.csr_matrix]:\n",
    "        \"\"\"Esegue generalizzazione ottimizzata basata sui COUNT e restituisce entrambe le matrici\"\"\"\n",
    "        print(f\"ðŸš€ Avvio generalizzazione ottimizzata basata sui COUNT (k={self.k_threshold})\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.initialize_optimized_matrix()\n",
    "        \n",
    "        step_count = 0\n",
    "        self.step_rates = []\n",
    "        \n",
    "        while True:\n",
    "            min_value = self.get_min_value_sparse()  # Basato sui COUNT\n",
    "            current_shape = self.current_matrix_sparse.shape\n",
    "            \n",
    "            print(f\"ðŸ” Step {step_count+1}: Min Count={min_value:,}, Shape={current_shape[0]}Ã—{current_shape[1]}, NonZero={self.current_matrix_sparse.nnz:,}\")\n",
    "        \n",
    "            # Controllo se tutti i COUNT sono >= k_threshold\n",
    "            min_after_generalization = self.current_matrix_sparse.data.min() if self.current_matrix_sparse.nnz > 0 else float('inf')\n",
    "            if min_after_generalization >= self.k_threshold:\n",
    "                print(f\"âœ… Tutti i count >= k ({self.k_threshold}), generalizzazione completata.\")\n",
    "                break\n",
    "            \n",
    "            # âš–ï¸ Calcolo del rapporto originale e bilanciamento dinamico\n",
    "            if step_count == 0:\n",
    "                initial_cols = self.current_matrix_sparse.shape[1]\n",
    "                initial_rows = self.current_matrix_sparse.shape[0]\n",
    "                self.initial_ratio = initial_cols / initial_rows if initial_rows > 0 else 1.0\n",
    "                self.tolerance = 0.03  # accetta Â±3% deviazione\n",
    "\n",
    "            current_cols = self.current_matrix_sparse.shape[1]\n",
    "            current_rows = self.current_matrix_sparse.shape[0]\n",
    "            current_ratio = current_cols / current_rows if current_rows > 0 else 1.0\n",
    "\n",
    "            # Calcola deviazione percentuale dal rapporto iniziale\n",
    "            deviation = (current_ratio - self.initial_ratio) / self.initial_ratio\n",
    "\n",
    "            # Se fuori tolleranza, forza generalizzazione sul lato dominante\n",
    "            if deviation > self.tolerance:\n",
    "                axis = 'columns'  # troppe colonne â‡’ generalizza colonne\n",
    "            elif deviation < -self.tolerance:\n",
    "                axis = 'rows'     # troppe righe â‡’ generalizza righe\n",
    "            else:\n",
    "                # dentro la tolleranza â‡’ alterna normalmente\n",
    "                axis = 'columns' if step_count % 2 == 0 else 'rows'\n",
    "            \n",
    "            best_gen = self.get_best_generalization_fast(axis)\n",
    "            if not best_gen:\n",
    "                # Prova sulla direzione opposta\n",
    "                fallback_axis = 'rows' if axis == 'columns' else 'columns'\n",
    "                best_gen = self.get_best_generalization_fast(fallback_axis)\n",
    "                if best_gen:\n",
    "                    axis = fallback_axis\n",
    "                    print(f\"ðŸ”„ Nessuna generalizzazione per la direzione originale, faccio fallback sulla direzione: {fallback_axis}\")\n",
    "                else:\n",
    "                    print(f\"âš ï¸ Nessuna generalizzazione possibile nÃ© su {axis} nÃ© sulla direzione alternativa ({fallback_axis}).\")\n",
    "                    if min_after_generalization < self.k_threshold:\n",
    "                        print(f\"âŒ Esistono ancora count < k ({self.k_threshold}), ma impossibile generalizzare ulteriormente.\")\n",
    "                    break\n",
    "\n",
    "            group, parent_id, cost = best_gen\n",
    "            \n",
    "            # Applica generalizzazione su entrambe le matrici\n",
    "            self.apply_sparse_generalization(group, parent_id, axis)\n",
    "\n",
    "            if current_shape[0] <= 50 or current_shape[1] <= 50:\n",
    "                print(\"ðŸ§¾ Matrice count corrente (valori interi non-zero):\")\n",
    "                print(self.current_matrix_sparse.toarray().astype(int))\n",
    "                print(\"ðŸ§¾ Matrice pesi corrente (valori interi non-zero):\")\n",
    "                print(self.current_weights_sparse.toarray().astype(int))\n",
    "            \n",
    "            # Registra\n",
    "            self.generalization_history.append({\n",
    "                'step': step_count + 1,\n",
    "                'axis': axis,\n",
    "                'group_size': len(group),\n",
    "                'parent': parent_id,\n",
    "                'cost': cost,\n",
    "                'min_before': min_value,\n",
    "                'shape_after': self.current_matrix_sparse.shape\n",
    "            })\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            step_rate = step_count / elapsed if elapsed > 0 else 0\n",
    "            self.step_rates.append(step_rate)\n",
    "            step_count += 1\n",
    "            \n",
    "            # Stampa progresso ogni 10 step\n",
    "            if step_count % 10 == 0:\n",
    "                print(f\"â±ï¸ Elapsed: {elapsed:.1f}s, Step rate: {step_rate:.2f} steps/sec\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nðŸŽ¯ Generalizzazione completata in {step_count} passi ({total_time:.2f}s)\")\n",
    "        print(f\"ðŸ“ Matrice finale: {self.current_matrix_sparse.shape}\")\n",
    "        print(f\"ðŸ“Š Threshold da rispettare: {self.k_threshold}\")\n",
    "        print(f\"ðŸ“Š Valore minimo finale (count): {self.current_matrix_sparse.data.min() if self.current_matrix_sparse.nnz > 0 else 'N/A'}\")\n",
    "        print(f\"ðŸ“Š Valore massimo finale (count): {self.current_matrix_sparse.data.max() if self.current_matrix_sparse.nnz > 0 else 'N/A'}\")\n",
    "        print(f\"ðŸ“Š Valore minimo finale (pesi): {self.current_weights_sparse.data.min() if self.current_weights_sparse.nnz > 0 else 'N/A'}\")\n",
    "        print(f\"ðŸ“Š Valore massimo finale (pesi): {self.current_weights_sparse.data.max() if self.current_weights_sparse.nnz > 0 else 'N/A'}\")\n",
    "        print(f\"ðŸ’¾ Elementi non-zero: {self.current_matrix_sparse.nnz:,}\")\n",
    "        \n",
    "        return self.current_matrix_sparse, self.current_weights_sparse\n",
    "\n",
    "def run_optimized_generalization(od_matrix_df: pd.DataFrame, tree_start, tree_end, k_threshold: int = 10):\n",
    "    \"\"\"\n",
    "    Esegue la generalizzazione ottimizzata basata sui COUNT per k-anonimity\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[sp.csr_matrix, sp.csr_matrix, OptimizedH3GeneralizedODMatrix]: \n",
    "        (matrice_count, matrice_pesi, generalizer)\n",
    "    \"\"\"\n",
    "    generalizer = OptimizedH3GeneralizedODMatrix(od_matrix_df, tree_start, tree_end, k_threshold)\n",
    "    count_result, weights_result = generalizer.run_optimized_generalization()\n",
    "    \n",
    "    return count_result, weights_result, generalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e481770",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_result, weights_result, generalizer = run_optimized_generalization(od_matrix, tree_start, tree_end, k_threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58237615",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valore minimo in weights_result:\", weights_result.data.min())\n",
    "print(\"Valore massimo in weights_result:\", weights_result.data.max())\n",
    "\n",
    "k = 10*media_peso\n",
    "less_than_k = (weights_result.data < k).sum()\n",
    "print(f\"Celle con meno di {k} record aggregati:\", less_than_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53157179",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountAnalyzer:\n",
    "    \"\"\"\n",
    "    Analizzatore per la k-anonimitÃ  e distribuzione dei count e pesi dopo generalizzazione\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, counts_matrix: sp.csr_matrix, weights_matrix: sp.csr_matrix, \n",
    "                 generalizer, k_count: int, k_weight: float):\n",
    "        self.weights_matrix = weights_matrix\n",
    "        self.counts_matrix = counts_matrix\n",
    "        self.generalizer = generalizer\n",
    "        self.k_count = k_count\n",
    "        self.k_weight = k_weight\n",
    "\n",
    "    def analyze_count_anonymity(self) -> Dict:\n",
    "        counts_coo = self.counts_matrix.tocoo()\n",
    "        count_values = counts_coo.data\n",
    "        stats = {\n",
    "            'total_non_zero_cells': len(count_values),\n",
    "            'min_count': int(count_values.min()) if len(count_values) > 0 else 0,\n",
    "            'max_count': int(count_values.max()) if len(count_values) > 0 else 0,\n",
    "            'mean_count': float(count_values.mean()) if len(count_values) > 0 else 0,\n",
    "            'median_count': float(np.median(count_values)) if len(count_values) > 0 else 0,\n",
    "            'std_count': float(count_values.std()) if len(count_values) > 0 else 0\n",
    "        }\n",
    "        below_k = count_values[count_values < self.k_count]\n",
    "        stats['cells_below_k'] = len(below_k)\n",
    "        stats['percent_below_k'] = (len(below_k) / len(count_values) * 100) if len(count_values) > 0 else 0\n",
    "        stats['is_k_anonymous'] = len(below_k) == 0\n",
    "        unique_counts, frequencies = np.unique(count_values, return_counts=True)\n",
    "        stats['unique_count_values'] = len(unique_counts)\n",
    "        stats['most_common_count'] = int(unique_counts[np.argmax(frequencies)])\n",
    "        stats['most_common_frequency'] = int(frequencies.max())\n",
    "        return stats\n",
    "\n",
    "    def analyze_weight_anonymity(self) -> Dict:\n",
    "        weights_coo = self.weights_matrix.tocoo()\n",
    "        weight_values = weights_coo.data\n",
    "        stats = {\n",
    "            'total_non_zero_cells': len(weight_values),\n",
    "            'min_weight': float(weight_values.min()) if len(weight_values) > 0 else 0,\n",
    "            'max_weight': float(weight_values.max()) if len(weight_values) > 0 else 0,\n",
    "            'mean_weight': float(weight_values.mean()) if len(weight_values) > 0 else 0,\n",
    "            'median_weight': float(np.median(weight_values)) if len(weight_values) > 0 else 0,\n",
    "            'std_weight': float(weight_values.std()) if len(weight_values) > 0 else 0\n",
    "        }\n",
    "        below_k = weight_values[weight_values < self.k_weight]\n",
    "        stats['cells_below_k'] = len(below_k)\n",
    "        stats['percent_below_k'] = (len(below_k) / len(weight_values) * 100) if len(weight_values) > 0 else 0\n",
    "        stats['is_k_anonymous'] = len(below_k) == 0\n",
    "        return stats\n",
    "\n",
    "    def print_summary_report(self):\n",
    "        count_stats = self.analyze_count_anonymity()\n",
    "        print(f\"\\nðŸ”¢ COUNT (k={self.k_count})\")\n",
    "        print(f\"   Celle sotto soglia k: {count_stats['cells_below_k']:,}\")\n",
    "        print(f\"   Percentuale sotto k: {count_stats['percent_below_k']:.2f}%\")\n",
    "        print(f\"   Ãˆ k-anonima? {'âœ… SÃŒ' if count_stats['is_k_anonymous'] else 'âŒ NO'}\")\n",
    "        weight_stats = self.analyze_weight_anonymity()\n",
    "        print(f\"\\nâš–ï¸  PESI (k={self.k_weight:.2f})\")\n",
    "        print(f\"   Celle sotto soglia k: {weight_stats['cells_below_k']:,}\")\n",
    "        print(f\"   Percentuale sotto k: {weight_stats['percent_below_k']:.2f}%\")\n",
    "        print(f\"   Ãˆ k-anonima? {'âœ… SÃŒ' if weight_stats['is_k_anonymous'] else 'âŒ NO'}\")\n",
    "        return count_stats, weight_stats\n",
    "\n",
    "k_count = 10\n",
    "k_weight = 10 * media_peso\n",
    "\n",
    "analyzer = CountAnalyzer(sparse_result, weights_result, generalizer, k_count, k_weight)\n",
    "count_stats, weight_stats = analyzer.print_summary_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def plot_count_distributions(weights_result, k_count=10*media_peso):\n",
    "    \"\"\"\n",
    "    Plotta due istogrammi affiancati:\n",
    "    - Tutta la distribuzione dei count (log-scale)\n",
    "    - Zoom sui count tra 0 e 200\n",
    "    \"\"\"\n",
    "    counts_coo = coo_matrix(weights_result)\n",
    "    counts = counts_coo.data\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Istogramma completo (log scale)\n",
    "    axes[0].hist(counts, bins=50, color='skyblue', edgecolor='black', log=True)\n",
    "    axes[0].axvline(k_count, color='red', linestyle='--', linewidth=2, label=f'k = {k_count}')\n",
    "    axes[0].set_title('Distribuzione completa dei Pesi')\n",
    "    axes[0].set_xlabel('Valore di pesi')\n",
    "    axes[0].set_ylabel('Frequenza (log)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Zoom tra 0 e 200\n",
    "    counts_zoom = counts[counts <= k_count*1.2]\n",
    "    axes[1].hist(counts_zoom, bins=40, color='mediumseagreen', edgecolor='black')\n",
    "    axes[1].axvline(k_count, color='red', linestyle='--', linewidth=2, label=f'k = {k_count}')\n",
    "    axes[1].set_title('Zoom dei pesi')\n",
    "    axes[1].set_xlabel('Valore di pesi')\n",
    "    axes[1].set_ylabel('Frequenza')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_count_distributions(weights_result, k_count=10*media_peso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ee67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import h3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import scipy.sparse as sp\n",
    "from branca.colormap import linear\n",
    "import json\n",
    "from folium.plugins import HeatMap\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "class H3FoliumVisualizer:\n",
    "    \"\"\"\n",
    "    Classe per visualizzare i risultati della generalizzazione H3 con Folium\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, generalizer, center_lat=48.8566, center_lon=2.3522):\n",
    "        \"\"\"\n",
    "        Inizializza il visualizzatore\n",
    "        \n",
    "        Args:\n",
    "            generalizer: Istanza di OptimizedH3GeneralizedODMatrix\n",
    "            center_lat, center_lon: Coordinate del centro della mappa (default: Torino)\n",
    "        \"\"\"\n",
    "        self.generalizer = generalizer\n",
    "        self.center_lat = center_lat\n",
    "        self.center_lon = center_lon\n",
    "        self.sparse_matrix = generalizer.current_matrix_sparse\n",
    "        \n",
    "        # Estrai dati dalla matrice sparse per visualizzazione\n",
    "        self._extract_visualization_data()\n",
    "    \n",
    "    def _extract_visualization_data(self):\n",
    "        print(\"ðŸ“Š Estrazione dati per visualizzazione...\")\n",
    "        coo = self.sparse_matrix.tocoo()\n",
    "\n",
    "        # Prepara liste di mapping con None per indici mancanti\n",
    "        list_idx_to_start = [None] * self.sparse_matrix.shape[1]\n",
    "        for idx, h3_id in self.generalizer.idx_to_start.items():\n",
    "            list_idx_to_start[idx] = h3_id\n",
    "\n",
    "        list_idx_to_end = [None] * self.sparse_matrix.shape[0]\n",
    "        for idx, h3_id in self.generalizer.idx_to_end.items():\n",
    "            list_idx_to_end[idx] = h3_id\n",
    "\n",
    "        self.origin_data = {}\n",
    "        self.destination_data = {}\n",
    "        self.od_pairs = []\n",
    "\n",
    "        # Calcola flussi totali per origine\n",
    "        for start_idx, h3_id in enumerate(list_idx_to_start):\n",
    "            if h3_id is None:\n",
    "                continue\n",
    "            total_flow = self.sparse_matrix[:, start_idx].sum()\n",
    "            if total_flow > 0:\n",
    "                self.origin_data[h3_id] = total_flow\n",
    "\n",
    "        # Calcola flussi totali per destinazione\n",
    "        for end_idx, h3_id in enumerate(list_idx_to_end):\n",
    "            if h3_id is None:\n",
    "                continue\n",
    "            total_flow = self.sparse_matrix[end_idx, :].sum()\n",
    "            if total_flow > 0:\n",
    "                self.destination_data[h3_id] = total_flow\n",
    "\n",
    "        # Estrai coppie OD\n",
    "        for i, j, data in zip(coo.row, coo.col, coo.data):\n",
    "            if data > 0:\n",
    "                if j < len(list_idx_to_start) and i < len(list_idx_to_end):\n",
    "                    origin_h3 = list_idx_to_start[j]\n",
    "                    dest_h3 = list_idx_to_end[i]\n",
    "                    if origin_h3 is not None and dest_h3 is not None:\n",
    "                        self.od_pairs.append((origin_h3, dest_h3, data))\n",
    "\n",
    "        print(f\"âœ… Estratti {len(self.origin_data)} origini, {len(self.destination_data)} destinazioni, {len(self.od_pairs)} coppie OD\")\n",
    "\n",
    "    def _h3_to_geojson(self, h3_id: str) -> Dict:\n",
    "        \"\"\"Converte un esagono H3 in GeoJSON\"\"\"\n",
    "        boundary = h3.cell_to_boundary(h3_id)\n",
    "        # H3 restituisce (lat, lon), GeoJSON vuole (lon, lat)\n",
    "        coords = [[[lon, lat] for lat, lon in boundary]]\n",
    "        \n",
    "        return {\n",
    "            \"type\": \"Feature\",\n",
    "            \"geometry\": {\n",
    "                \"type\": \"Polygon\",\n",
    "                \"coordinates\": coords\n",
    "            },\n",
    "            \"properties\": {\n",
    "                \"h3_id\": h3_id,\n",
    "                \"resolution\": h3.get_resolution(h3_id)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def create_base_map(self, zoom_start=10) -> folium.Map:\n",
    "        \"\"\"Crea la mappa base\"\"\"\n",
    "        m = folium.Map(\n",
    "            location=[self.center_lat, self.center_lon],\n",
    "            zoom_start=zoom_start,\n",
    "            tiles='OpenStreetMap'\n",
    "        )\n",
    "        \n",
    "        # Aggiungi controlli layer\n",
    "        folium.plugins.Fullscreen().add_to(m)\n",
    "        \n",
    "        return m\n",
    "    \n",
    "    def add_origin_hexagons(self, m: folium.Map, max_hexagons=100, alpha=0.6):\n",
    "        \"\"\"Aggiunge gli esagoni di origine alla mappa\"\"\"\n",
    "        print(f\"ðŸ”µ Aggiunta esagoni origine (max {max_hexagons})...\")\n",
    "        \n",
    "        # Ordina per flusso e prendi i top\n",
    "        sorted_origins = sorted(self.origin_data.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # k = self.generalizer.k_threshold\n",
    "        # sorted_origins = [(h3_id, flow) for h3_id, flow in sorted_origins if flow >= k]\n",
    "\n",
    "        top_origins = sorted_origins[:max_hexagons]\n",
    "        \n",
    "        if not top_origins:\n",
    "            print(\"âš ï¸ Nessun esagono origine da visualizzare\")\n",
    "            return\n",
    "        \n",
    "        # Calcola range per normalizzazione colori\n",
    "        flows = [flow for _, flow in top_origins]\n",
    "        min_flow, max_flow = min(flows), max(flows)\n",
    "        \n",
    "        # Gruppo layer per le origini\n",
    "        origin_group = folium.FeatureGroup(name=f'Origini (Top {len(top_origins)})', show=True)\n",
    "        \n",
    "        for h3_id, flow in top_origins:\n",
    "            try:\n",
    "                # Ottieni geometria\n",
    "                geojson = self._h3_to_geojson(h3_id)\n",
    "                \n",
    "                # Calcola intensitÃ  colore normalizzata\n",
    "                if max_flow > min_flow:\n",
    "                    intensity = (flow - min_flow) / (max_flow - min_flow)\n",
    "                else:\n",
    "                    intensity = 1.0\n",
    "                \n",
    "                # Usa colori esadecimali invece di rgba per evitare problemi\n",
    "                # Calcola il blu con intensitÃ  variabile\n",
    "                blue_intensity = int(255 * (0.3 + intensity * 0.7))\n",
    "                fill_color = f\"#{0:02x}{0:02x}{blue_intensity:02x}\"\n",
    "                \n",
    "                # Aggiungi esagono con style_function semplificata\n",
    "                folium.GeoJson(\n",
    "                    geojson,\n",
    "                    style_function=lambda x: {\n",
    "                        'fillColor': fill_color,\n",
    "                        'color': 'darkblue',\n",
    "                        'weight': 1,\n",
    "                        'fillOpacity': alpha,\n",
    "                        'opacity': 0.8\n",
    "                    },\n",
    "                    popup=folium.Popup(\n",
    "                        f\"\"\"\n",
    "                        <b>Origine H3</b><br>\n",
    "                        ID: {h3_id}<br>\n",
    "                        Risoluzione: {h3.get_resolution(h3_id)}<br>\n",
    "                        Flusso totale: {flow:,}<br>\n",
    "                        Rank: {sorted_origins.index((h3_id, flow)) + 1}\n",
    "                        \"\"\",\n",
    "                        max_width=200\n",
    "                    ),\n",
    "                    tooltip=f\"Origine: {flow:,} viaggi\"\n",
    "                ).add_to(origin_group)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Errore nell'aggiungere esagono origine {h3_id}: {e}\")\n",
    "        \n",
    "        origin_group.add_to(m)\n",
    "        print(f\"âœ… Aggiunti {len(top_origins)} esagoni origine\")\n",
    "    \n",
    "    def add_destination_hexagons(self, m: folium.Map, max_hexagons=100, alpha=0.6):\n",
    "        \"\"\"Aggiunge gli esagoni di destinazione alla mappa\"\"\"\n",
    "        print(f\"ðŸ”´ Aggiunta esagoni destinazione (max {max_hexagons})...\")\n",
    "        \n",
    "        # Ordina per flusso e prendi i top\n",
    "        sorted_destinations = sorted(self.destination_data.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # k = self.generalizer.k_threshold\n",
    "        # sorted_destinations = [(h3_id, flow) for h3_id, flow in sorted_destinations if flow >= k]\n",
    "\n",
    "        top_destinations = sorted_destinations[:max_hexagons]\n",
    "        \n",
    "        if not top_destinations:\n",
    "            print(\"âš ï¸ Nessun esagono destinazione da visualizzare\")\n",
    "            return\n",
    "        \n",
    "        # Gruppo layer per le destinazioni\n",
    "        dest_group = folium.FeatureGroup(name=f'Destinazioni (Top {len(top_destinations)})', show=True)\n",
    "        \n",
    "        flows = [flow for _, flow in top_destinations]\n",
    "        min_flow, max_flow = min(flows), max(flows)\n",
    "        \n",
    "        for h3_id, flow in top_destinations:\n",
    "            try:\n",
    "                geojson = self._h3_to_geojson(h3_id)\n",
    "                \n",
    "                # Calcola intensitÃ  colore\n",
    "                if max_flow > min_flow:\n",
    "                    intensity = (flow - min_flow) / (max_flow - min_flow)\n",
    "                else:\n",
    "                    intensity = 1.0\n",
    "                \n",
    "                # Usa colori esadecimali per il rosso\n",
    "                red_intensity = int(255 * (0.3 + intensity * 0.7))\n",
    "                fill_color = f\"#{red_intensity:02x}{0:02x}{0:02x}\"\n",
    "                \n",
    "                folium.GeoJson(\n",
    "                    geojson,\n",
    "                    style_function=lambda x: {\n",
    "                        'fillColor': fill_color,\n",
    "                        'color': 'darkred',\n",
    "                        'weight': 1,\n",
    "                        'fillOpacity': alpha,\n",
    "                        'opacity': 0.8\n",
    "                    },\n",
    "                    popup=folium.Popup(\n",
    "                        f\"\"\"\n",
    "                        <b>Destinazione H3</b><br>\n",
    "                        ID: {h3_id}<br>\n",
    "                        Risoluzione: {h3.get_resolution(h3_id)}<br>\n",
    "                        Flusso totale: {flow:,}<br>\n",
    "                        Rank: {sorted_destinations.index((h3_id, flow)) + 1}\n",
    "                        \"\"\",\n",
    "                        max_width=200\n",
    "                    ),\n",
    "                    tooltip=f\"Destinazione: {flow:,} viaggi\"\n",
    "                ).add_to(dest_group)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Errore nell'aggiungere esagono destinazione {h3_id}: {e}\")\n",
    "        \n",
    "        dest_group.add_to(m)\n",
    "        print(f\"âœ… Aggiunti {len(top_destinations)} esagoni destinazione\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50215beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = H3FoliumVisualizer(generalizer)\n",
    "\n",
    "mappa = visualizer.create_base_map(zoom_start=11)\n",
    "\n",
    "visualizer.add_origin_hexagons(mappa, max_hexagons=14000)\n",
    "visualizer.add_destination_hexagons(mappa, max_hexagons=14000)\n",
    "\n",
    "folium.LayerControl(collapsed=False).add_to(mappa)\n",
    "\n",
    "mappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e991bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCOLA C_DM e C_AVG\n",
    "# Equivalence classes sono le coppie origine/destinazione con count >= k\n",
    "# La loro grandezza Ã¨ il numero di count nella matrice per qualla coppia righe/colonne\n",
    "# C_DM : somma dei quadrati delle grandezze delle equivalence classes\n",
    "# C_AVG : (numero totale di record / numero di equivalence classes) / k\n",
    "\n",
    "def compute_discernability_and_cavg_sparse(sparse_matrix: sp.csr_matrix, od_matrix, suppressed_count: int, k: int) -> dict:\n",
    "    \n",
    "    counts = sparse_matrix.data\n",
    "    total_records = counts.sum() + suppressed_count\n",
    "    total_equiv_classes = sparse_matrix.nnz + suppressed_count\n",
    "\n",
    "    # Aggiungo una penalitÃ  per i record soppressi\n",
    "    # Ogni riga soppressa conta come una classe di equivalenza\n",
    "    # Quindi ogni riga ha una penalitÃ  grande quanto la grandezza del dataset\n",
    "    suppression_penalty = len(od_matrix) * suppressed_count\n",
    "\n",
    "    # C_DM: somma dei quadrati dei count >= k\n",
    "    k_anonymous_counts = counts[counts >= k]\n",
    "    c_dm_gen = (k_anonymous_counts ** 2).sum()\n",
    "    c_dm = c_dm_gen + suppression_penalty\n",
    "\n",
    "    # CAVG: ((total_records / total_equiv_classes) / k)    \n",
    "    c_avg = (total_records / total_equiv_classes) / k if total_equiv_classes > 0 else float('inf')\n",
    "\n",
    "    return {\n",
    "        'C_DM': c_dm,\n",
    "        'C_AVG': c_avg,\n",
    "        'total_records': total_records,\n",
    "        'total_equivalence_classes': total_equiv_classes,\n",
    "        'k': k\n",
    "    }\n",
    "\n",
    "metrics = compute_discernability_and_cavg_sparse(sparse_result, od_matrix, suppressed_count=suppressed_count, k=10)\n",
    "print(\"\\nðŸ“Š Metrics di DiscernibilitÃ  e CAVG:\")\n",
    "print(f\"C_DM: {metrics['C_DM']:,}\")\n",
    "print(f\"C_AVG: {metrics['C_AVG']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c175deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "def calculate_generalization_distance_metric(df_merged: pd.DataFrame, generalizer, tree_start, tree_end) -> Dict:\n",
    "    \n",
    "    print(\"ðŸ” Calcolo metrica di distanza post-generalizzazione...\")\n",
    "    \n",
    "    # 1. Ottieni le mappature finali dalla generalizzazione\n",
    "    final_start_mapping = generalizer.start_to_idx\n",
    "    final_end_mapping = generalizer.end_to_idx\n",
    "    \n",
    "    # 2. Crea mapping da esagoni originali a esagoni generalizzati\n",
    "    start_original_to_generalized = {}\n",
    "    end_original_to_generalized = {}\n",
    "    \n",
    "    # Per ogni esagono originale, trova l'esagono generalizzato corrispondente\n",
    "    # In 'start_original_to_generalized' e 'end_original_to_generalized' ci sono gli esagoni generalizzati\n",
    "    unique_start_h3 = df_merged['start_h3'].unique()\n",
    "    unique_end_h3 = df_merged['end_h3'].unique()\n",
    "    \n",
    "    print(f\"ðŸ“Š Mappatura {len(unique_start_h3)} esagoni origine...\")\n",
    "    for original_h3 in unique_start_h3:\n",
    "        generalized_h3 = find_generalized_hexagon(original_h3, final_start_mapping, tree_start)\n",
    "        if generalized_h3:\n",
    "            start_original_to_generalized[original_h3] = generalized_h3\n",
    "    \n",
    "    print(f\"ðŸ“Š Mappatura {len(unique_end_h3)} esagoni destinazione...\")\n",
    "    for original_h3 in unique_end_h3:\n",
    "        generalized_h3 = find_generalized_hexagon(original_h3, final_end_mapping, tree_end)\n",
    "        if generalized_h3:\n",
    "            end_original_to_generalized[original_h3] = generalized_h3\n",
    "    \n",
    "    # 3. Calcola distanze per i punti di partenza\n",
    "    # Recupera le coordinate originali e calcola la distanza tra quel punto e l'esagono generalizzato (il centro dell'esagono)\n",
    "    start_distances = []\n",
    "    start_coords = []\n",
    "    \n",
    "    for idx, row in df_merged.iterrows():\n",
    "        original_h3 = row['start_h3']\n",
    "        original_coords = (row['start_lat'], row['start_lon'])\n",
    "        \n",
    "        if original_h3 in start_original_to_generalized:\n",
    "            generalized_h3 = start_original_to_generalized[original_h3]\n",
    "            # Funzione utlizzata per trovare il centro di un esagono H3\n",
    "            generalized_coords = h3.cell_to_latlng(generalized_h3)\n",
    "            \n",
    "            # Distanza che tiene conto della curvatura della Terra (piÃ¹ precisa di una distanza euclidea che considera la Terra come piatta)\n",
    "            distance = geodesic(original_coords, generalized_coords).meters\n",
    "                \n",
    "            start_distances.append(distance)\n",
    "            start_coords.append({\n",
    "                'original_h3': original_h3,\n",
    "                'generalized_h3': generalized_h3,\n",
    "                'original_coords': original_coords,\n",
    "                'generalized_coords': generalized_coords,\n",
    "                'distance': distance\n",
    "            })\n",
    "    \n",
    "    # 4. Calcola distanze per i punti di destinazione\n",
    "    end_distances = []\n",
    "    end_coords = []\n",
    "    \n",
    "    for idx, row in df_merged.iterrows():\n",
    "        original_h3 = row['end_h3']\n",
    "        original_coords = (row['end_lat'], row['end_lon'])\n",
    "        \n",
    "        if original_h3 in end_original_to_generalized:\n",
    "            generalized_h3 = end_original_to_generalized[original_h3]\n",
    "            generalized_coords = h3.cell_to_latlng(generalized_h3)\n",
    "            \n",
    "            distance = geodesic(original_coords, generalized_coords).meters\n",
    "                \n",
    "            end_distances.append(distance)\n",
    "            end_coords.append({\n",
    "                'original_h3': original_h3,\n",
    "                'generalized_h3': generalized_h3,\n",
    "                'original_coords': original_coords,\n",
    "                'generalized_coords': generalized_coords,\n",
    "                'distance': distance\n",
    "            })\n",
    "    \n",
    "    # 5. Calcola statistiche\n",
    "    results = {\n",
    "        'start_distances': {\n",
    "            'mean': np.mean(start_distances) if start_distances else 0,\n",
    "            'median': np.median(start_distances) if start_distances else 0,\n",
    "            'std': np.std(start_distances) if start_distances else 0,\n",
    "            'min': np.min(start_distances) if start_distances else 0,\n",
    "            'max': np.max(start_distances) if start_distances else 0,\n",
    "            'count': len(start_distances)\n",
    "        },\n",
    "        'end_distances': {\n",
    "            'mean': np.mean(end_distances) if end_distances else 0,\n",
    "            'median': np.median(end_distances) if end_distances else 0,\n",
    "            'std': np.std(end_distances) if end_distances else 0,\n",
    "            'min': np.min(end_distances) if end_distances else 0,\n",
    "            'max': np.max(end_distances) if end_distances else 0,\n",
    "            'count': len(end_distances)\n",
    "        },\n",
    "        'overall': {\n",
    "            'mean': np.mean(start_distances + end_distances) if (start_distances or end_distances) else 0,\n",
    "            'median': np.median(start_distances + end_distances) if (start_distances or end_distances) else 0,\n",
    "            'std': np.std(start_distances + end_distances) if (start_distances or end_distances) else 0,\n",
    "            'total_points': len(start_distances) + len(end_distances)\n",
    "        },\n",
    "        'mappings': {\n",
    "            'start_original_to_generalized': start_original_to_generalized,\n",
    "            'end_original_to_generalized': end_original_to_generalized\n",
    "        },\n",
    "        'detailed_coords': {\n",
    "            'start': start_coords,\n",
    "            'end': end_coords\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 6. Stampa risultati\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ“ METRICHE DI DISTANZA POST-GENERALIZZAZIONE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ PUNTI DI PARTENZA:\")\n",
    "    print(f\"   â€¢ Distanza media: {results['start_distances']['mean']:.2f} metri\")\n",
    "    print(f\"   â€¢ Distanza mediana: {results['start_distances']['median']:.2f} metri\")\n",
    "    print(f\"   â€¢ Deviazione standard: {results['start_distances']['std']:.2f} metri\")\n",
    "    print(f\"   â€¢ Min-Max: {results['start_distances']['min']:.2f} - {results['start_distances']['max']:.2f} metri\")\n",
    "    print(f\"   â€¢ Punti analizzati: {results['start_distances']['count']:,}\")\n",
    "    \n",
    "    print(f\"\\nðŸ PUNTI DI DESTINAZIONE:\")\n",
    "    print(f\"   â€¢ Distanza media: {results['end_distances']['mean']:.2f} metri\")\n",
    "    print(f\"   â€¢ Distanza mediana: {results['end_distances']['median']:.2f} metri\")\n",
    "    print(f\"   â€¢ Deviazione standard: {results['end_distances']['std']:.2f} metri\")\n",
    "    print(f\"   â€¢ Min-Max: {results['end_distances']['min']:.2f} - {results['end_distances']['max']:.2f} metri\")\n",
    "    print(f\"   â€¢ Punti analizzati: {results['end_distances']['count']:,}\")\n",
    "    \n",
    "    print(f\"\\nðŸŒ COMPLESSIVO:\")\n",
    "    print(f\"   â€¢ Distanza media totale: {results['overall']['mean']:.2f} metri\")\n",
    "    print(f\"   â€¢ Distanza mediana totale: {results['overall']['median']:.2f} metri\")\n",
    "    print(f\"   â€¢ Deviazione standard totale: {results['overall']['std']:.2f} metri\")\n",
    "    print(f\"   â€¢ Punti totali: {results['overall']['total_points']:,}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def find_generalized_hexagon(original_h3: str, final_mapping: Dict, tree) -> str:\n",
    "    \"\"\"\n",
    "    Trova l'esagono generalizzato corrispondente a un esagono originale\n",
    "    \"\"\"\n",
    "    # Se l'esagono Ã¨ direttamente presente nella mappatura finale\n",
    "    if original_h3 in final_mapping:\n",
    "        return original_h3\n",
    "    \n",
    "    # Se non trovato, cerca tra TUTTI gli esagoni finali se l'originale Ã¨ loro discendente\n",
    "    for final_h3 in final_mapping.keys():\n",
    "        if is_descendant_of(original_h3, final_h3):\n",
    "            return final_h3\n",
    "    \n",
    "    return None\n",
    "\n",
    "def is_descendant_of(child_h3: str, parent_h3: str) -> bool:\n",
    "    \"\"\"\n",
    "    Controlla se child_h3 Ã¨ discendente di parent_h3\n",
    "    \"\"\"\n",
    "    child_res = h3.get_resolution(child_h3)\n",
    "    parent_res = h3.get_resolution(parent_h3)\n",
    "    \n",
    "    if parent_res >= child_res:\n",
    "        return False\n",
    "    \n",
    "    current = child_h3\n",
    "    while h3.get_resolution(current) > parent_res:\n",
    "        current = h3.cell_to_parent(current, h3.get_resolution(current) - 1)\n",
    "    \n",
    "    return current == parent_h3\n",
    "\n",
    "def analyze_generalization_impact(results: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Analizza l'impatto della generalizzazione sulle distanze\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ“Š ANALISI IMPATTO GENERALIZZAZIONE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Calcola percentili\n",
    "    all_distances = []\n",
    "    for coord in results['detailed_coords']['start'] + results['detailed_coords']['end']:\n",
    "        all_distances.append(coord['distance'])\n",
    "    \n",
    "    if all_distances:\n",
    "        percentiles = [25, 50, 75, 90, 95, 99]\n",
    "        print(\"\\nðŸ“ Distribuzione distanze:\")\n",
    "        for p in percentiles:\n",
    "            value = np.percentile(all_distances, p)\n",
    "            print(f\"   â€¢ {p}Â° percentile: {value:.2f} metri\")\n",
    "    \n",
    "    # Analizza per risoluzione\n",
    "    resolution_analysis = {}\n",
    "    for coord in results['detailed_coords']['start'] + results['detailed_coords']['end']:\n",
    "        original_res = h3.get_resolution(coord['original_h3'])\n",
    "        generalized_res = h3.get_resolution(coord['generalized_h3'])\n",
    "        \n",
    "        key = f\"{original_res}â†’{generalized_res}\"\n",
    "        if key not in resolution_analysis:\n",
    "            resolution_analysis[key] = []\n",
    "        resolution_analysis[key].append(coord['distance'])\n",
    "    \n",
    "    print(\"\\nðŸ” Analisi per risoluzione:\")\n",
    "    for resolution_change, distances in resolution_analysis.items():\n",
    "        # Distanza media per questa tipologia di generalizzazione\n",
    "        mean_dist = np.mean(distances)\n",
    "        # Numero di punti che hanno subito quella generalizzazione\n",
    "        count = len(distances)\n",
    "        print(f\"   â€¢ {resolution_change}: {mean_dist:.2f}m (n={count})\")\n",
    "\n",
    "\n",
    "distance_results = calculate_generalization_distance_metric(\n",
    "    df_merged=filtered_df, \n",
    "    generalizer=generalizer,\n",
    "    tree_start=tree_start,\n",
    "    tree_end=tree_end\n",
    ")\n",
    "\n",
    "analyze_generalization_impact(distance_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91abdbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mean Generalisation Error : per ogni flusso anonimizzato, conta quanti esagoni originali sono inclusi nell'area generalizzata (sia per origine che destinazione)\n",
    "# \n",
    "# PAPER:\n",
    "# Quando le origini e le destinazioni sono aggregate piÃ¹ o meno allo stesso livello (come di solito accade),\n",
    "# il valore medio ðº rappresenta circa il doppio del numero di celle (esagoni) coinvolte \n",
    "# nell'origine o nella destinazione per ogni flusso generalizzato.\n",
    "# Un valore di ðº = 2 significa quindi che non Ã¨ stata effettuata alcuna generalizzazione.\n",
    "\n",
    "class GeneralizationMetric:\n",
    "    \"\"\"\n",
    "    á¸  = (1/V+) Ã— Î£(|o| + |d|) Ã— v_{oâ†’d}\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, k_threshold: int = 10):\n",
    "        self.k_threshold = k_threshold\n",
    "        \n",
    "    def calculate_generalization_error(self, generalized_matrix: sp.csr_matrix, generalizer: 'OptimizedH3GeneralizedODMatrix') -> float:\n",
    "                \n",
    "        matrix_dense = generalized_matrix.toarray()\n",
    "        return self._calculate_dense(matrix_dense, generalizer)\n",
    "    \n",
    "    def _calculate_dense(self, matrix: np.ndarray, generalizer) -> float:\n",
    "        \n",
    "        # Denominatore sommatoria dei flussi >= k (V+)\n",
    "        total_volume_anonymous = 0  \n",
    "        # Numeratore (Î£(|o| + |d|) * v_oâ†’d)\n",
    "        weighted_count_sum = 0\n",
    "        \n",
    "        rows, cols = matrix.shape\n",
    "        \n",
    "        # Scorriamo la matrice OD\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                flow_value = matrix[i, j]\n",
    "                \n",
    "                # Considera solo i flussi >= k_threshold\n",
    "                if flow_value >= self.k_threshold:\n",
    "                    # Ottieni gli ID H3 dalle mappature\n",
    "                    origin_h3 = generalizer.idx_to_start.get(j)\n",
    "                    destination_h3 = generalizer.idx_to_end.get(i)\n",
    "                    \n",
    "                    if origin_h3 and destination_h3:\n",
    "                        # '_get_node_count' dice quante celle originali sono state aggregate dentro ciascun nodo H3 generalizzato\n",
    "                        origin_count = self._get_node_count(origin_h3, generalizer.tree_start)\n",
    "                        destination_count = self._get_node_count(destination_h3, generalizer.tree_end)\n",
    "                        \n",
    "                        # Sommo i count per ottenere V+\n",
    "                        total_volume_anonymous += flow_value\n",
    "                        # Calcola il contributo del numeratore\n",
    "                        weighted_count_sum += (origin_count + destination_count) * flow_value\n",
    "        \n",
    "        # Calcola l'errore medio\n",
    "        if total_volume_anonymous > 0:\n",
    "            mean_generalization_error = weighted_count_sum / total_volume_anonymous\n",
    "        else:\n",
    "            mean_generalization_error = 0.0\n",
    "            \n",
    "        return mean_generalization_error\n",
    "    \n",
    "    def _get_node_count(self, h3_id: str, tree) -> int:\n",
    "        \"\"\"\n",
    "        Dato un nodo H3 generalizzato, conta quante celle originali (foglie) rappresenta.\n",
    "        \"\"\"\n",
    "        # Caso limite : il nodo non Ã¨ nell'albero\n",
    "        if h3_id not in tree.nodes:\n",
    "            return 0\n",
    "        \n",
    "        node = tree.nodes[h3_id]\n",
    "        \n",
    "        # Se Ã¨ una foglia (livello piÃ¹ basso), conta 1 => l'esagono generalizzato rappresenta una cella originale\n",
    "        if not node.children:\n",
    "            return 1\n",
    "        \n",
    "        # Altrimenti, chiama _count_terminal_nodes per contare tutte le foglie sotto questo nodo\n",
    "        return self._count_terminal_nodes(node, tree)\n",
    "    \n",
    "    def _count_terminal_nodes(self, node, tree) -> int:\n",
    "        \"\"\"\n",
    "        Conta ricorsivamente tutti i nodi terminali sotto un nodo.\n",
    "        \"\"\"\n",
    "        if not node.children:\n",
    "            return 1\n",
    "        \n",
    "        total = 0\n",
    "        for child_id in node.children:\n",
    "            if child_id in tree.nodes:\n",
    "                child_node = tree.nodes[child_id]\n",
    "                total += self._count_terminal_nodes(child_node, tree)\n",
    "            \n",
    "        return total\n",
    "    \n",
    "metric = GeneralizationMetric(k_threshold=10)\n",
    "error = metric.calculate_generalization_error(sparse_result, generalizer)\n",
    "    \n",
    "print(f\"Errore di generalizzazione medio GÌ„: {error:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b76a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte dal nodo h3_id\n",
    "# Se Ã¨ un nodo foglia (nessn figlio/discnedente), lo aggiunge al set\n",
    "# Se ha figli, li esplora ricorsivamente\n",
    "# Il risultato sono tutti gli esagoni foglia che sono stati aggregati in h3_id\n",
    "\n",
    "def get_leaf_descendants(h3_id: str, tree) -> set:\n",
    "    \"\"\"\n",
    "    Ritorna tutti i nodi foglia discendenti di un nodo H3.\n",
    "    \"\"\"\n",
    "    leaves = set()\n",
    "\n",
    "    def _collect(node_id):\n",
    "        node = tree.nodes.get(node_id)\n",
    "        if not node or not node.children:\n",
    "            leaves.add(node_id)\n",
    "        else:\n",
    "            for child_id in node.children:\n",
    "                _collect(child_id)\n",
    "\n",
    "    _collect(h3_id)\n",
    "    return leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05f33b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per ogni nodo generalizzato\n",
    "# Estrae tutti i suoi discendenti foglia\n",
    "# E li associa nella forma:\n",
    "# {\n",
    "#     foglia_1: generalizzato_A,\n",
    "#     foglia_2: generalizzato_A,\n",
    "#     ...\n",
    "# }\n",
    "\n",
    "generalizer.start_generalization_map = {\n",
    "    leaf_h3: gen_h3 for gen_h3 in generalizer.start_to_idx\n",
    "    for leaf_h3 in get_leaf_descendants(gen_h3, tree_start)\n",
    "}\n",
    "\n",
    "generalizer.end_generalization_map = {\n",
    "    leaf_h3: gen_h3 for gen_h3 in generalizer.end_to_idx\n",
    "    for leaf_h3 in get_leaf_descendants(gen_h3, tree_end)\n",
    "}\n",
    "\n",
    "generalizer.start_idx_map = {h3: idx for h3, idx in generalizer.start_to_idx.items()}\n",
    "generalizer.end_idx_map = {h3: idx for h3, idx in generalizer.end_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048bcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction loss : differenza assoluta media tra i flussi originali e quelli ricostruiti, normalizzata per il volume totale\n",
    "\n",
    "def fast_reconstruction_loss(original_od_df: pd.DataFrame,\n",
    "                             generalized_matrix: sp.csr_matrix,\n",
    "                             generalizer) -> float:\n",
    "    \"\"\"\n",
    "    Calcola la reconstruction loss:\n",
    "    E = (1/V) * Î£ |á¹½_oâ†’d - v_oâ†’d|\n",
    "    \"\"\"\n",
    "    total_volume = original_od_df['count'].sum()\n",
    "    total_abs_error = 0\n",
    "    \n",
    "    for _, row in original_od_df.iterrows():\n",
    "        start_h3 = row['start_h3']\n",
    "        end_h3 = row['end_h3']\n",
    "        true_count = row['count']\n",
    "        \n",
    "        # Trova mapping generalizzato\n",
    "        gen_start = generalizer.start_generalization_map.get(start_h3)\n",
    "        gen_end = generalizer.end_generalization_map.get(end_h3)\n",
    "        \n",
    "        if gen_start is None or gen_end is None:\n",
    "            # Record soppresso: errore = volume completo\n",
    "            total_abs_error += true_count\n",
    "            continue\n",
    "        \n",
    "        # Trova indici matrice\n",
    "        row_idx = generalizer.end_idx_map.get(gen_end)\n",
    "        col_idx = generalizer.start_idx_map.get(gen_start)\n",
    "        \n",
    "        if row_idx is None or col_idx is None:\n",
    "            # Record soppresso: errore = volume completo\n",
    "            total_abs_error += true_count\n",
    "            continue\n",
    "        \n",
    "        # Record generalizzato: calcola errore normale\n",
    "        gen_count = generalized_matrix[row_idx, col_idx]\n",
    "        total_abs_error += abs(gen_count - true_count)\n",
    "    \n",
    "    return total_abs_error / total_volume if total_volume > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79df611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = fast_reconstruction_loss(original_od_df=od_matrix_first, generalized_matrix=sparse_result, generalizer=generalizer)\n",
    "print(f\"Reconstruction Loss: {loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8353e852",
   "metadata": {},
   "source": [
    "### Metriche con pesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc18bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCOLA C_DM e C_AVG\n",
    "# Equivalence classes sono le coppie origine/destinazione con count >= k\n",
    "# La loro grandezza Ã¨ il numero di count nella matrice per qualla coppia righe/colonne\n",
    "# C_DM : somma dei quadrati delle grandezze delle equivalence classes\n",
    "# C_AVG : (numero totale di record / numero di equivalence classes) / k\n",
    "\n",
    "metrics = compute_discernability_and_cavg_sparse(weights_result, od_matrix, suppressed_count=suppressed_count, k=10*media_peso)\n",
    "print(\"\\nðŸ“Š Metrics di DiscernibilitÃ  e CAVG:\")\n",
    "print(f\"C_DM: {metrics['C_DM']:,}\")\n",
    "print(f\"C_AVG: {metrics['C_AVG']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99365f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mean Generalisation Error : per ogni flusso anonimizzato, conta quanti esagoni originali sono inclusi nell'area generalizzata (sia per origine che destinazione)\n",
    "# \n",
    "# PAPER:\n",
    "# Quando le origini e le destinazioni sono aggregate piÃ¹ o meno allo stesso livello (come di solito accade),\n",
    "# il valore medio ðº rappresenta circa il doppio del numero di celle (esagoni) coinvolte \n",
    "# nell'origine o nella destinazione per ogni flusso generalizzato.\n",
    "# Un valore di ðº = 2 significa quindi che non Ã¨ stata effettuata alcuna generalizzazione.\n",
    "    \n",
    "metric = GeneralizationMetric(k_threshold=10*media_peso)\n",
    "error = metric.calculate_generalization_error(weights_result, generalizer)\n",
    "    \n",
    "print(f\"Errore di generalizzazione medio GÌ„: {error:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f696848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = fast_reconstruction_loss(original_od_df=od_matrix_first, generalized_matrix=weights_result, generalizer=generalizer)\n",
    "print(f\"Reconstruction Loss: {loss:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
