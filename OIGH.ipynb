{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e19397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import folium\n",
    "import hdbscan\n",
    "import h3\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pl\n",
    "import matplotlib.patches as patches\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from typing import Dict, Set, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c989ce-bee1-4a32-94bf-936c752b7a75",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "310627af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gps = pd.read_csv(\"data/processed_trips.csv\")\n",
    "\n",
    "df_people = pd.read_csv(\"data/individuals_dataset.csv\")\n",
    "df_people = df_people[df_people['GPS_RECORD'] == True]\n",
    "\n",
    "df_merged = pd.merge(\n",
    "    df_gps,\n",
    "    df_people[['ID', 'WEIGHT_INDIV']],\n",
    "    left_on='ID',\n",
    "    right_on='ID',\n",
    "    how='inner'\n",
    ")#.drop(columns='ID')\n",
    "df_merged = df_merged.rename(columns={\"ori_lat\": \"start_lat\", \"ori_lon\": \"start_lon\", \"dst_lat\":'end_lat',\"dst_lon\":'end_lon'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4ad59a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_resolution = 10\n",
    "df_merged['start_h3'] = df_merged.apply(lambda row: h3.latlng_to_cell(row['start_lat'], row['start_lon'], h3_resolution), axis=1)\n",
    "df_merged['end_h3'] = df_merged.apply(lambda row: h3.latlng_to_cell(row['end_lat'], row['end_lon'], h3_resolution), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd1526c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "od_matrix_first = df_merged.groupby(['start_h3', 'end_h3']).agg({\n",
    "    'WEIGHT_INDIV': ['sum', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "od_matrix_first.columns = ['start_h3', 'end_h3', 'total_weight', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77062b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of filtered rows: 23,264\n"
     ]
    }
   ],
   "source": [
    "#Keep only OD pairs within central paris area\n",
    "\n",
    "# Single hexagon for paris (res 5): \"851fb467fffffff\"\n",
    "# 4 children hexagon covering central Paris (res=6): \"861fb4667ffffff\", \"861fb4677ffffff\", \"861fb466fffffff\", \"861fb4647ffffff\"\n",
    "# res=7 \"871fb4674ffffff\", \"871fb475bffffff\", \"871fb4675ffffff\", \"871fb4666ffffff\", \"871fb4662ffffff\", \"871fb4660ffffff\"\n",
    "# \"861fb4667ffffff\", \"861fb4677ffffff\", \"861fb466fffffff\", \"861fb4647ffffff\", \"861fb4297ffffff\", \"861fb474fffffff\", \"861fb475fffffff\", \"861fb462fffffff\"\n",
    "parent_hexes = [\"861fb4667ffffff\", \"861fb4677ffffff\", \"861fb466fffffff\", \"861fb4647ffffff\", \"861fb475fffffff\"]\n",
    "\n",
    "# Generate the list of children at resolution 10\n",
    "target_resolution = 10\n",
    "start_valid_h3 = set()\n",
    "end_valid_h3 = set()\n",
    "\n",
    "for parent in parent_hexes:\n",
    "    children = h3.cell_to_children(parent, target_resolution)  # <-- NON serve compact\n",
    "    for child in children:\n",
    "        start_valid_h3.add(child)\n",
    "        end_valid_h3.add(child)\n",
    "mask = (\n",
    "    (od_matrix_first[\"start_h3\"].isin(start_valid_h3))\n",
    "    & (od_matrix_first[\"end_h3\"].isin(end_valid_h3))\n",
    ")\n",
    "od_matrix_first = od_matrix_first[mask].copy()\n",
    "print(f\"Number of filtered rows: {len(od_matrix_first):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06e4c97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "od_matrix = od_matrix_first.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6359783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_merged.merge(\n",
    "    od_matrix[['start_h3', 'end_h3']],\n",
    "    on=['start_h3', 'end_h3'],\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecc992ea-d67a-45d2-9dcb-651a9f42f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class H3TreeNode:\n",
    "    \"\"\"H3 hierarchy tree node\"\"\"\n",
    "    def __init__(self, h3_id: str, resolution: int, total_weight: int = 0, count: int = 0):\n",
    "        self.h3_id = h3_id\n",
    "        self.resolution = resolution\n",
    "        self.total_weight = total_weight\n",
    "        self.count = count\n",
    "        self.children: Dict[str, 'H3TreeNode'] = {}\n",
    "        self.parent: Optional['H3TreeNode'] = None\n",
    "    \n",
    "    def add_child(self, child: 'H3TreeNode'):\n",
    "        \"\"\"Adds a child to the node\"\"\"\n",
    "        self.children[child.h3_id] = child\n",
    "        child.parent = self\n",
    "    \n",
    "    def add_weight(self, weight: int):\n",
    "        self.total_weight += weight\n",
    "        if self.parent:\n",
    "            self.parent.add_weight(weight)\n",
    "    \n",
    "    def add_count(self, count: int):\n",
    "        self.count += count\n",
    "        if self.parent:\n",
    "            self.parent.add_count(count)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (f\"H3Node(id={self.h3_id}, res={self.resolution}, \"\n",
    "                f\"total_weight={self.total_weight}, count={self.count}, children={len(self.children)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d4e2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class H3HierarchicalTree:\n",
    "    def __init__(self, od_matrix: pd.DataFrame, target_resolution: int = 11, hex_column: str = 'start_h3'):\n",
    "        self.od_matrix = od_matrix\n",
    "        self.target_resolution = target_resolution\n",
    "        self.hex_column = hex_column  # 'start_h3' o 'end_h3'\n",
    "        self.nodes: Dict[str, H3TreeNode] = {}\n",
    "        self.root = None\n",
    "        self.min_resolution = None  # Sarà calcolato dinamicamente\n",
    "        \n",
    "    def get_all_hexagons(self) -> Set[str]:\n",
    "        \"\"\"Extracts all unique hexagons from the specified column\"\"\"\n",
    "        return set(self.od_matrix[self.hex_column].unique())\n",
    "    \n",
    "    def get_resolution_coverage(self, hexagons: Set[str], target_res: int) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Obtains all target resolution hexagons covering the area defined by the input hexagons.\n",
    "        \"\"\"\n",
    "        coverage_hexagons = set()\n",
    "        \n",
    "        for hex_id in hexagons:\n",
    "            current_res = h3.get_resolution(hex_id)\n",
    "            \n",
    "            if current_res == target_res:\n",
    "                coverage_hexagons.add(hex_id)\n",
    "            elif current_res < target_res:\n",
    "                # Dobbiamo espandere a risoluzione più alta\n",
    "                children = self._get_all_children_at_resolution(hex_id, target_res)\n",
    "                coverage_hexagons.update(children)\n",
    "            else:\n",
    "                # Dobbiamo salire a risoluzione più bassa\n",
    "                parent = h3.cell_to_parent(hex_id, target_res)\n",
    "                coverage_hexagons.add(parent)\n",
    "        \n",
    "        return coverage_hexagons\n",
    "    \n",
    "    def find_optimal_min_resolution(self, hexagons: Set[str]) -> int:\n",
    "        \"\"\"\n",
    "        Find the highest resolution where there is still only one node covering all hexagons.\n",
    "        \"\"\"        \n",
    "        # For each resolution from 0 to the target, count how many nodes are needed to cover all hexagons.\n",
    "        resolution_stats = {}\n",
    "        \n",
    "        for resolution in range(0, self.target_resolution + 1):\n",
    "            ancestors = set()\n",
    "            for hex_id in hexagons:\n",
    "                current_res = h3.get_resolution(hex_id)\n",
    "                if current_res >= resolution:\n",
    "                    ancestor = h3.cell_to_parent(hex_id, resolution)\n",
    "                    ancestors.add(ancestor)\n",
    "                else:\n",
    "                    # add the hexagon directly if already with a lower resolution, \n",
    "                    ancestors.add(hex_id)\n",
    "            \n",
    "            resolution_stats[resolution] = len(ancestors)\n",
    "            print(f\"Resolution {resolution}: {len(ancestors)} nodes\")\n",
    "        \n",
    "        # Find the highest resolution with count = 1\n",
    "        optimal_resolution = 0\n",
    "        for resolution in range(self.target_resolution, -1, -1):\n",
    "            if resolution_stats[resolution] == 1:\n",
    "                optimal_resolution = resolution\n",
    "                break\n",
    "        \n",
    "        # print(f\"optimal resolution: {optimal_resolution}\")\n",
    "        return optimal_resolution\n",
    "    \n",
    "    def get_siblings(self, node_id: str) -> List[str]:\n",
    "        \"\"\"Returns the h3_ids of the sibling nodes of node_id (other hexagons within the same parent)\"\"\"\n",
    "        if node_id not in self.nodes:\n",
    "            return []\n",
    "\n",
    "        node = self.nodes[node_id]\n",
    "        parent = node.parent\n",
    "        if parent is None:\n",
    "            return []\n",
    "\n",
    "        siblings = [child.h3_id for child in parent.children.values() if child.h3_id != node_id]\n",
    "        return siblings\n",
    "    \n",
    "    def get_parent(self, node_id: str) -> Optional[str]:\n",
    "        \"\"\"Returns the h3_id of the parent node, or None for root node.\"\"\"\n",
    "        if node_id not in self.nodes:\n",
    "            return None\n",
    "        node = self.nodes[node_id]\n",
    "        if node.parent is None:\n",
    "            return None\n",
    "        return node.parent.h3_id\n",
    "    \n",
    "    def _get_all_children_at_resolution(self, hex_id: str, target_res: int) -> Set[str]:\n",
    "        \"\"\"Recursively obtains all children at a specific resolution\"\"\"\n",
    "        current_res = h3.get_resolution(hex_id)\n",
    "        \n",
    "        if current_res == target_res:\n",
    "            return {hex_id}\n",
    "        elif current_res > target_res:\n",
    "            return set()\n",
    "        \n",
    "        children = set()\n",
    "        direct_children = h3.cell_to_children(hex_id, current_res + 1)\n",
    "        \n",
    "        for child in direct_children:\n",
    "            children.update(self._get_all_children_at_resolution(child, target_res))\n",
    "        \n",
    "        return children\n",
    "    \n",
    "    def build_hierarchy_path(self, hex_id: str, min_resolution: int) -> List[str]:\n",
    "        \"\"\"Builds the hierarchical path from a hexagon to the minimum resolution\"\"\"\n",
    "        path = [hex_id]\n",
    "        current = hex_id\n",
    "        current_res = h3.get_resolution(current)\n",
    "        \n",
    "        while current_res > min_resolution:\n",
    "            parent = h3.cell_to_parent(current, current_res - 1)\n",
    "            path.append(parent)\n",
    "            current = parent\n",
    "            current_res -= 1\n",
    "        \n",
    "        return path\n",
    "    \n",
    "    def create_tree_structure(self):\n",
    "        \"\"\"Create the optimized tree structure\"\"\"\n",
    "        target_hexagons = self.get_all_hexagons()\n",
    "        \n",
    "        # Get full coverage at the target resolution\n",
    "        coverage_hexagons = self.get_resolution_coverage(target_hexagons, self.target_resolution)\n",
    "        \n",
    "        # Find the optimal minimum resolution (the highest one with count=1)\n",
    "        self.min_resolution = self.find_optimal_min_resolution(coverage_hexagons)\n",
    "        \n",
    "        # print(f\"resolution tree from {self.min_resolution} to {self.target_resolution}\")\n",
    "        \n",
    "        # Build all hierarchical paths\n",
    "        all_paths = []\n",
    "        for hex_id in coverage_hexagons:\n",
    "            path = self.build_hierarchy_path(hex_id, self.min_resolution)\n",
    "            all_paths.append(path)\n",
    "        \n",
    "        for path in all_paths:\n",
    "            for hex_id in path:\n",
    "                if hex_id not in self.nodes:\n",
    "                    resolution = h3.get_resolution(hex_id)\n",
    "                    self.nodes[hex_id] = H3TreeNode(hex_id, resolution)\n",
    "        \n",
    "        # parent-child relation\n",
    "        for path in all_paths:\n",
    "            for i in range(len(path) - 1):\n",
    "                child_id = path[i]\n",
    "                parent_id = path[i + 1]\n",
    "                self.nodes[parent_id].add_child(self.nodes[child_id])\n",
    "        \n",
    "        # Identify the root\n",
    "        self.root = self.nodes[self._find_root_hexagon(coverage_hexagons, self.min_resolution)]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _find_root_hexagon(self, hexagons: Set[str], min_resolution: int) -> str:\n",
    "        sample_hex = next(iter(hexagons))\n",
    "        return h3.cell_to_parent(sample_hex, min_resolution)\n",
    "    \n",
    "    def populate_counts(self):\n",
    "        # Group the two metrics by hexagon\n",
    "        agg_df = self.od_matrix.groupby(self.hex_column).agg({'total_weight': 'sum', 'count': 'sum'}).reset_index()\n",
    "        \n",
    "        for _, row in agg_df.iterrows():\n",
    "            hex_id = row[self.hex_column]\n",
    "            weight = int(row['total_weight'])\n",
    "            count = int(row['count'])\n",
    "            \n",
    "            target_hex = self._map_to_target_resolution(hex_id)\n",
    "            \n",
    "            if target_hex in self.nodes:\n",
    "                self.nodes[target_hex].add_weight(weight)\n",
    "                self.nodes[target_hex].add_count(count)\n",
    "            else:\n",
    "                print(f\"hexagon {target_hex} not found\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _map_to_target_resolution(self, hex_id: str) -> str:\n",
    "        \"\"\"Map a hexagon to the target resolution\"\"\"\n",
    "        current_res = h3.get_resolution(hex_id)\n",
    "        \n",
    "        if current_res == self.target_resolution:\n",
    "            return hex_id\n",
    "        elif current_res < self.target_resolution:\n",
    "            # Take the first child available\n",
    "            children = self._get_all_children_at_resolution(hex_id, self.target_resolution)\n",
    "            return next(iter(children)) if children else hex_id\n",
    "        else:\n",
    "            return h3.cell_to_parent(hex_id, self.target_resolution)\n",
    "    \n",
    "    def get_tree_statistics(self) -> Dict:\n",
    "        if not self.root:\n",
    "            return {}\n",
    "        \n",
    "        stats = {\n",
    "            'total_nodes': len(self.nodes),\n",
    "            'root_resolution': self.root.resolution,\n",
    "            'min_resolution': self.min_resolution,\n",
    "            'target_resolution': self.target_resolution,\n",
    "            'total_weight': self.root.total_weight,\n",
    "            'nodes_by_resolution': defaultdict(int),\n",
    "            'resolution_range': f\"{self.min_resolution} → {self.target_resolution}\"\n",
    "        }\n",
    "        \n",
    "        for node in self.nodes.values():\n",
    "            stats['nodes_by_resolution'][node.resolution] += 1\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def print_tree(self, max_children_per_level: int = 10):\n",
    "        if not self.root:\n",
    "            print(\"Tree not built\")\n",
    "            return\n",
    "        \n",
    "        print(f\"OPTIMIZED TREE STRUCTURE: resolution range {self.min_resolution} → {self.target_resolution}\")\n",
    "        \n",
    "        def _print_node(node: H3TreeNode, depth: int = 0, is_last: bool = True, prefix: str = \"\"):\n",
    "            connector = \"└─ \" if is_last else \"├─ \"\n",
    "            print(f\"{prefix}{connector}{node.h3_id} (res:{node.resolution}, total_weight:{node.total_weight}, count:{node.count}, children:{len(node.children)})\")\n",
    "            \n",
    "            if is_last:\n",
    "                child_prefix = prefix + \"   \"\n",
    "            else:\n",
    "                child_prefix = prefix + \"│  \"\n",
    "            \n",
    "            children_list = list(node.children.values())\n",
    "            \n",
    "            if len(children_list) <= max_children_per_level:\n",
    "                for i, child in enumerate(children_list):\n",
    "                    is_last_child = (i == len(children_list) - 1)\n",
    "                    _print_node(child, depth + 1, is_last_child, child_prefix)\n",
    "            else:\n",
    "                for i in range(max_children_per_level):\n",
    "                    child = children_list[i]\n",
    "                    is_last_child = (i == max_children_per_level - 1) and (len(children_list) == max_children_per_level)\n",
    "                    _print_node(child, depth + 1, is_last_child, child_prefix)\n",
    "                \n",
    "                remaining = len(children_list) - max_children_per_level\n",
    "                print(f\"{child_prefix}└─ ... and other {remaining} children with the same pattern\")\n",
    "        \n",
    "        _print_node(self.root, 0, True, \"\")\n",
    "\n",
    "def create_h3_hierarchical_tree(od_matrix_df: pd.DataFrame, target_resolution: int = 10, hex_column: str = 'start_h3'):\n",
    "    \"\"\"\n",
    "    Create an optimized H3 hierarchical tree from the OD matrix dataset\n",
    "    \n",
    "    Args:\n",
    "        od_matrix_df: DataFrame with column 'start_h3', 'end_h3', 'count'\n",
    "        target_resolution: Target resolution for tree leaves\n",
    "        hex_column: Column to be analyzed ('start_h3' o 'end_h3')\n",
    "    \n",
    "    Returns:\n",
    "        H3HierarchicalTree: Constructed and optimized hierarchical tree\n",
    "    \"\"\"\n",
    "    \n",
    "    tree = H3HierarchicalTree(od_matrix_df, target_resolution, hex_column)\n",
    "    tree.create_tree_structure()\n",
    "    tree.populate_counts()\n",
    "    \n",
    "    stats = tree.get_tree_statistics()\n",
    "    print(f\"OPTIMIZED TREE STATISTICS ({hex_column.upper()})\")\n",
    "    print(stats)\n",
    "    \n",
    "    # Calculate savings in nodes\n",
    "    total_resolutions_possible = target_resolution + 1  # da 0 a target\n",
    "    resolutions_used = len(stats['nodes_by_resolution'])\n",
    "    resolutions_saved = total_resolutions_possible - resolutions_used\n",
    "    \n",
    "    print(f\"OPTIMIZATIONS:\")\n",
    "    print(f\"Saved resolutions: {resolutions_saved}\")\n",
    "    print(f\"Tree efficiency: {resolutions_used}/{total_resolutions_possible} livelli utilizzati\")\n",
    "    \n",
    "    tree.print_tree()\n",
    "    \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4334675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_start = create_h3_hierarchical_tree(od_matrix, target_resolution=10, hex_column='start_h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea7ceb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_end = create_h3_hierarchical_tree(od_matrix, target_resolution=10, hex_column='end_h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a54ed5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd7943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1938c231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2138bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lattice2DCount:\n",
    "    \"\"\"\n",
    "    OIGH adattata per H3HierarchicalTree\n",
    "    k-anonimita' basata su count, total_weight viene aggregato insieme\n",
    "    \"\"\"\n",
    "    def __init__(self, od_matrix, tree_start, tree_end, k, S):\n",
    "        self.od_matrix = od_matrix.copy()\n",
    "        self.tree_start = tree_start\n",
    "        self.tree_end = tree_end\n",
    "        self.k = k\n",
    "        self.S = S\n",
    "        self.total_vol = self.od_matrix['count'].sum()\n",
    "\n",
    "        self.L_start = max(node.resolution for node in tree_start.nodes.values())\n",
    "        self.L_end   = max(node.resolution for node in tree_end.nodes.values())\n",
    "\n",
    "        self.nodes = {}\n",
    "        self.add_node(self.L_start, self.L_end, parents=[])\n",
    "        self.max_level_found = np.inf\n",
    "        self.od_matrix_agg = None\n",
    "        self.best_avg_class_size = np.inf\n",
    "\n",
    "    def add_node(self, lvlo, lvld, parents):\n",
    "        if (lvlo, lvld) not in self.nodes:\n",
    "            node = LatticeNodeCount(lvlo, lvld, parents=parents, lattice=self)\n",
    "            self.nodes[(lvlo, lvld)] = node\n",
    "            if lvlo > self.tree_start.min_resolution:\n",
    "                self.add_node(lvlo-1, lvld, [node])\n",
    "                node.children.append(self.nodes[(lvlo-1, lvld)])\n",
    "            if lvld > self.tree_end.min_resolution:\n",
    "                self.add_node(lvlo, lvld-1, [node])\n",
    "                node.children.append(self.nodes[(lvlo, lvld-1)])\n",
    "        else:\n",
    "            self.nodes[(lvlo, lvld)].parents += parents\n",
    "\n",
    "\n",
    "class LatticeNodeCount:\n",
    "    def __init__(self, lvlo, lvld, parents, lattice):\n",
    "        self.lattice = lattice\n",
    "        self.lvlo = lvlo\n",
    "        self.lvld = lvld\n",
    "        self.parents = parents\n",
    "        self.children = []\n",
    "        self.anonymous = None\n",
    "        self.visited = False\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.visited = True\n",
    "        if self.lvlo + self.lvld <= self.lattice.max_level_found:\n",
    "\n",
    "            if self.children and not self.children[0].visited:\n",
    "                self.children[0].evaluate()\n",
    "\n",
    "            if self.anonymous is None:\n",
    "                od_matrix_agg = self.get_aggregation()\n",
    "\n",
    "                self.avg_class_size = self.get_mean_agg_level(od_matrix_agg)\n",
    "                self.suppr_vol = od_matrix_agg[od_matrix_agg['count'] < self.lattice.k]['count'].sum()\n",
    "\n",
    "                if self.suppr_vol > self.lattice.S:\n",
    "                    self.tag_unanonymous()\n",
    "                else:\n",
    "                    if self.lvlo + self.lvld == self.lattice.max_level_found:\n",
    "                        if (self.lattice.od_matrix_agg is None) or (self.avg_class_size < self.lattice.best_avg_class_size):\n",
    "                            self.lattice.od_matrix_agg = od_matrix_agg\n",
    "                            self.lattice.best_avg_class_size = self.avg_class_size\n",
    "                    else:\n",
    "                        self.lattice.max_level_found = self.lvlo + self.lvld\n",
    "                        self.lattice.od_matrix_agg = od_matrix_agg\n",
    "                        self.lattice.best_avg_class_size = self.avg_class_size\n",
    "                    self.tag_anonymous()\n",
    "\n",
    "            if len(self.children) > 1 and not self.children[1].visited:\n",
    "                self.children[1].evaluate()\n",
    "\n",
    "    def tag_anonymous(self):\n",
    "        if self.anonymous is None:\n",
    "            self.anonymous = True\n",
    "            for c in self.children:\n",
    "                c.tag_anonymous()\n",
    "\n",
    "    def tag_unanonymous(self):\n",
    "        if self.anonymous is None:\n",
    "            self.anonymous = False\n",
    "            for p in self.parents:\n",
    "                p.tag_unanonymous()\n",
    "\n",
    "    def map_to_level(self, h, target_res, tree):\n",
    "        node = tree.nodes.get(h)\n",
    "        if node is None:\n",
    "            return h\n",
    "        while node and node.resolution > target_res:\n",
    "            node = node.parent\n",
    "        return node.h3_id if node else h\n",
    "\n",
    "    def get_aggregation(self):\n",
    "        df = self.lattice.od_matrix.copy()\n",
    "\n",
    "        # aggrega start e end\n",
    "        df['start_gen'] = df['start_h3'].apply(lambda h: self.map_to_level(h, self.lvlo, self.lattice.tree_start))\n",
    "        df['end_gen']   = df['end_h3'].apply(lambda h: self.map_to_level(h, self.lvld, self.lattice.tree_end))\n",
    "\n",
    "        # aggrega count e total_weight\n",
    "        agg = df.groupby(['start_gen', 'end_gen']).agg({\n",
    "            'count': 'sum',          # k-anonimity basata su count\n",
    "            'total_weight': 'sum'    # aggregazione total_weight\n",
    "        }).reset_index()\n",
    "\n",
    "        return agg\n",
    "\n",
    "    def get_mean_agg_level(self, od_matrix_agg):\n",
    "        def weighted_res(h3_col, tree):\n",
    "            levels = []\n",
    "            for h in h3_col:\n",
    "                node = tree.nodes.get(h)\n",
    "                levels.append(node.resolution if node else tree.min_resolution)\n",
    "            return levels\n",
    "\n",
    "        od_matrix_agg['res_o'] = weighted_res(od_matrix_agg['start_gen'], self.lattice.tree_start)\n",
    "        od_matrix_agg['res_d'] = weighted_res(od_matrix_agg['end_gen'], self.lattice.tree_end)\n",
    "\n",
    "        od_matrix_agg['w_res_o'] = od_matrix_agg['res_o'] * od_matrix_agg['count']\n",
    "        od_matrix_agg['w_res_d'] = od_matrix_agg['res_d'] * od_matrix_agg['count']\n",
    "\n",
    "        mean_vals = od_matrix_agg[od_matrix_agg['count'] >= self.lattice.k].agg({\n",
    "            'w_res_o':'sum', 'w_res_d':'sum', 'count':'sum'\n",
    "        })\n",
    "\n",
    "        if mean_vals['count'] == 0:\n",
    "            return np.inf\n",
    "\n",
    "        return (mean_vals['w_res_o'] + mean_vals['w_res_d']) / mean_vals['count']\n",
    "\n",
    "\n",
    "def oigh(od_matrix, tree_start, tree_end, k, S):\n",
    "    lat = Lattice2DCount(od_matrix, tree_start, tree_end, k, S)\n",
    "    lat.nodes[(lat.L_start, lat.L_end)].evaluate()\n",
    "    return lat.od_matrix_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de2894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "od_matrix_generalized = oigh(od_matrix, tree_start, tree_end, k=10, S=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a61e65-42bf-4be6-9f51-8ede8c642473",
   "metadata": {},
   "outputs": [],
   "source": [
    "od_matrix_generalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c6f01c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralizedH3Visualizer:\n",
    "    def __init__(self, od_matrix, center_lat=48.8566, center_lon=2.3522):\n",
    "        \"\"\"\n",
    "        Visualize the generalized OD matrix H3 on Folium..\n",
    "\n",
    "        Args:\n",
    "            od_matrix: DataFrame with columns ['start_h3', 'end_h3', 'count']\n",
    "            center_lat, center_lon: map center coordinates\n",
    "        \"\"\"\n",
    "        self.od_matrix = od_matrix\n",
    "        self.center_lat = center_lat\n",
    "        self.center_lon = center_lon\n",
    "        \n",
    "        # Totali per origine e destinazione\n",
    "        self.origin_flows = od_matrix.groupby('start_gen')['count'].sum().to_dict()\n",
    "        self.dest_flows = od_matrix.groupby('end_gen')['count'].sum().to_dict()\n",
    "    \n",
    "    def _h3_to_geojson(self, h3_id):\n",
    "        \"\"\"Convert H3 to GeoJSON\"\"\"\n",
    "        boundary = h3.cell_to_boundary(h3_id)\n",
    "        coords = [[[lon, lat] for lat, lon in boundary]]  # GeoJSON vuole lon, lat\n",
    "        return {\n",
    "            \"type\": \"Feature\",\n",
    "            \"geometry\": {\"type\": \"Polygon\", \"coordinates\": coords},\n",
    "            \"properties\": {\"h3_id\": h3_id, \"resolution\": h3.get_resolution(h3_id)}\n",
    "        }\n",
    "    \n",
    "    def create_map(self, max_hexagons=100, alpha=0.6, zoom_start=10):\n",
    "        \"\"\"Create the Folium map with origin and destination hexagons\"\"\"\n",
    "        m = folium.Map(location=[self.center_lat, self.center_lon], zoom_start=zoom_start, tiles='OpenStreetMap')\n",
    "        \n",
    "        # Layer Origin (blue)\n",
    "        origins_sorted = sorted(self.origin_flows.items(), key=lambda x: x[1], reverse=True)[:max_hexagons]\n",
    "        origin_layer = folium.FeatureGroup(name=\"Origin (blue)\", show=True)\n",
    "        if origins_sorted:\n",
    "            min_flow, max_flow = min(v for _, v in origins_sorted), max(v for _, v in origins_sorted)\n",
    "            for h3_id, count in origins_sorted:\n",
    "                geojson = self._h3_to_geojson(h3_id)\n",
    "                intensity = (count - min_flow) / (max_flow - min_flow) if max_flow > min_flow else 1.0\n",
    "                blue_intensity = int(255 * (0.3 + 0.7*intensity))\n",
    "                fill_color = f\"#{0:02x}{0:02x}{blue_intensity:02x}\"\n",
    "                folium.GeoJson(\n",
    "                    geojson,\n",
    "                    style_function=lambda x, fill_color=fill_color: {\n",
    "                        'fillColor': fill_color,\n",
    "                        'color': 'darkblue',\n",
    "                        'weight': 1,\n",
    "                        'fillOpacity': alpha,\n",
    "                        'opacity': 0.8\n",
    "                    },\n",
    "                    tooltip=f\"{count} viaggi\"\n",
    "                ).add_to(origin_layer)\n",
    "        origin_layer.add_to(m)\n",
    "        \n",
    "        # Layer Destination (red)\n",
    "        dest_sorted = sorted(self.dest_flows.items(), key=lambda x: x[1], reverse=True)[:max_hexagons]\n",
    "        dest_layer = folium.FeatureGroup(name=\"Destination (red)\", show=True)\n",
    "        if dest_sorted:\n",
    "            min_flow, max_flow = min(v for _, v in dest_sorted), max(v for _, v in dest_sorted)\n",
    "            for h3_id, count in dest_sorted:\n",
    "                geojson = self._h3_to_geojson(h3_id)\n",
    "                intensity = (count - min_flow) / (max_flow - min_flow) if max_flow > min_flow else 1.0\n",
    "                red_intensity = int(255 * (0.3 + 0.7*intensity))\n",
    "                fill_color = f\"#{red_intensity:02x}{0:02x}{0:02x}\"\n",
    "                folium.GeoJson(\n",
    "                    geojson,\n",
    "                    style_function=lambda x, fill_color=fill_color: {\n",
    "                        'fillColor': fill_color,\n",
    "                        'color': 'darkred',\n",
    "                        'weight': 1,\n",
    "                        'fillOpacity': alpha,\n",
    "                        'opacity': 0.8\n",
    "                    },\n",
    "                    tooltip=f\"{count} viaggi\"\n",
    "                ).add_to(dest_layer)\n",
    "        dest_layer.add_to(m)\n",
    "        \n",
    "        folium.LayerControl().add_to(m)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e957238",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = GeneralizedH3Visualizer(od_matrix_generalized)\n",
    "mappa = visualizer.create_map(max_hexagons=2000000)\n",
    "mappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f3a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_discernability_and_cavg(df: pd.DataFrame, k: int, suppressed_count: int = 0) -> dict:\n",
    "    \"\"\"\n",
    "    compute C_DM e C_AVG for dataset OD generalization.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with column ['start_h3', 'end_h3', 'count']\n",
    "        k: for k-anonimity\n",
    "        suppressed_count: number of OD pairs suppressed (optional)\n",
    "    \n",
    "    \"\"\"\n",
    "    counts = df['count'].values\n",
    "    total_records = counts.sum() + suppressed_count\n",
    "    total_equiv_classes = len(counts) + suppressed_count\n",
    "    \n",
    "    # C_DM: somma dei quadrati dei count >= k\n",
    "    k_anonymous_counts = counts[counts >= k]\n",
    "    c_dm_gen = np.sum(k_anonymous_counts**2)\n",
    "    \n",
    "    # Penalità per record soppressi\n",
    "    suppression_penalty = suppressed_count * counts.sum()  # o totale record, a seconda della definizione\n",
    "    c_dm = c_dm_gen + suppression_penalty\n",
    "    \n",
    "    # C_AVG: (total_records / total_equiv_classes) / k\n",
    "    c_avg = (total_records / total_equiv_classes) / k if total_equiv_classes > 0 else float('inf')\n",
    "    \n",
    "    return {\n",
    "        'C_DM': c_dm,\n",
    "        'C_AVG': c_avg,\n",
    "        'total_records': total_records,\n",
    "        'total_equivalence_classes': total_equiv_classes,\n",
    "        'k': k\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0142a690-ef19-4280-a984-c878d6b46968",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_discernability_and_cavg(od_matrix_generalized, k=10, suppressed_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645c6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "def calculate_generalization_distance_metric(df: pd.DataFrame, od_matrix_generalized: pd.DataFrame) -> Dict:\n",
    "\n",
    "   # mapping from original hexagons to generalized hexagons\n",
    "   start_original_to_generalized = {}\n",
    "   end_original_to_generalized = {}\n",
    "   \n",
    "   # Get all unique generalized hexagons\n",
    "   generalized_start_h3 = set(od_matrix_generalized['start_gen'].unique())\n",
    "   generalized_end_h3 = set(od_matrix_generalized['end_gen'].unique())\n",
    "   \n",
    "   # For each original hexagon, find the corresponding generalized hexagon.\n",
    "   unique_start_h3 = df['start_h3'].unique()\n",
    "   unique_end_h3 = df['end_h3'].unique()\n",
    "   \n",
    "   for original_h3 in unique_start_h3:\n",
    "       generalized_h3 = find_generalized_hexagon(original_h3, generalized_start_h3)\n",
    "       if generalized_h3:\n",
    "           start_original_to_generalized[original_h3] = generalized_h3\n",
    "   \n",
    "   for original_h3 in unique_end_h3:\n",
    "       generalized_h3 = find_generalized_hexagon(original_h3, generalized_end_h3)\n",
    "       if generalized_h3:\n",
    "           end_original_to_generalized[original_h3] = generalized_h3\n",
    "   \n",
    "   # Calculate distances for origin points\n",
    "   start_distances = []\n",
    "   start_coords = []\n",
    "   \n",
    "   for idx, row in df.iterrows():\n",
    "       original_h3 = row['start_h3']\n",
    "       original_coords = (row['start_lat'], row['start_lon'])\n",
    "       \n",
    "       if original_h3 in start_original_to_generalized:\n",
    "           generalized_h3 = start_original_to_generalized[original_h3]\n",
    "           generalized_coords = h3.cell_to_latlng(generalized_h3)\n",
    "           \n",
    "           distance = geodesic(original_coords, generalized_coords).meters\n",
    "           \n",
    "           start_distances.append(distance)\n",
    "           start_coords.append({\n",
    "               'original_h3': original_h3,\n",
    "               'generalized_h3': generalized_h3,\n",
    "               'original_coords': original_coords,\n",
    "               'generalized_coords': generalized_coords,\n",
    "               'distance': distance\n",
    "           })\n",
    "   \n",
    "   # Calculate distances for destination points\n",
    "   end_distances = []\n",
    "   end_coords = []\n",
    "   \n",
    "   for idx, row in df.iterrows():\n",
    "       original_h3 = row['end_h3']\n",
    "       original_coords = (row['end_lat'], row['end_lon'])\n",
    "       \n",
    "       if original_h3 in end_original_to_generalized:\n",
    "           generalized_h3 = end_original_to_generalized[original_h3]\n",
    "           generalized_coords = h3.cell_to_latlng(generalized_h3)\n",
    "           \n",
    "           distance = geodesic(original_coords, generalized_coords).meters\n",
    "               \n",
    "           end_distances.append(distance)\n",
    "           end_coords.append({\n",
    "               'original_h3': original_h3,\n",
    "               'generalized_h3': generalized_h3,\n",
    "               'original_coords': original_coords,\n",
    "               'generalized_coords': generalized_coords,\n",
    "               'distance': distance\n",
    "           })\n",
    "   \n",
    "   results = {\n",
    "       'start_distances': {\n",
    "           'mean': np.mean(start_distances) if start_distances else 0,\n",
    "           'median': np.median(start_distances) if start_distances else 0,\n",
    "           'std': np.std(start_distances) if start_distances else 0,\n",
    "           'min': np.min(start_distances) if start_distances else 0,\n",
    "           'max': np.max(start_distances) if start_distances else 0,\n",
    "           'count': len(start_distances)\n",
    "       },\n",
    "       'end_distances': {\n",
    "           'mean': np.mean(end_distances) if end_distances else 0,\n",
    "           'median': np.median(end_distances) if end_distances else 0,\n",
    "           'std': np.std(end_distances) if end_distances else 0,\n",
    "           'min': np.min(end_distances) if end_distances else 0,\n",
    "           'max': np.max(end_distances) if end_distances else 0,\n",
    "           'count': len(end_distances)\n",
    "       },\n",
    "       'overall': {\n",
    "           'mean': np.mean(start_distances + end_distances) if (start_distances or end_distances) else 0,\n",
    "           'median': np.median(start_distances + end_distances) if (start_distances or end_distances) else 0,\n",
    "           'std': np.std(start_distances + end_distances) if (start_distances or end_distances) else 0,\n",
    "           'total_points': len(start_distances) + len(end_distances)\n",
    "       },\n",
    "       'mappings': {\n",
    "           'start_original_to_generalized': start_original_to_generalized,\n",
    "           'end_original_to_generalized': end_original_to_generalized\n",
    "       },\n",
    "       'detailed_coords': {\n",
    "           'start': start_coords,\n",
    "           'end': end_coords\n",
    "       }\n",
    "   }\n",
    "    return results\n",
    "\n",
    "def find_generalized_hexagon(original_h3: str, generalized_hexagons: set) -> str:\n",
    "   \"\"\"\n",
    "   Find the generalized hexagon corresponding to an original hexagon\n",
    "   \"\"\"\n",
    "   # If the hexagon is already in the list of generalized hexagons\n",
    "   if original_h3 in generalized_hexagons:\n",
    "       return original_h3\n",
    "   \n",
    "   # Otherwise, search among all generalized hexagons to see if the original is their descendant.\n",
    "   for generalized_h3 in generalized_hexagons:\n",
    "       if is_descendant_of(original_h3, generalized_h3):\n",
    "           return generalized_h3\n",
    "   \n",
    "   return None\n",
    "\n",
    "def is_descendant_of(child_h3: str, parent_h3: str) -> bool:\n",
    "   \"\"\"\n",
    "   Controlla se child_h3 è discendente di parent_h3\n",
    "   \"\"\"\n",
    "   child_res = h3.get_resolution(child_h3)\n",
    "   parent_res = h3.get_resolution(parent_h3)\n",
    "   \n",
    "   if parent_res >= child_res:\n",
    "       return False\n",
    "   \n",
    "   current = child_h3\n",
    "   while h3.get_resolution(current) > parent_res:\n",
    "       current = h3.cell_to_parent(current, h3.get_resolution(current) - 1)\n",
    "   \n",
    "   return current == parent_h3\n",
    "\n",
    "def analyze_generalization_impact(results: Dict) -> None:\n",
    "   \"\"\"\n",
    "   Analyze the impact of generalization on distances\n",
    "   \"\"\"\n",
    "   \n",
    "   all_distances = []\n",
    "   for coord in results['detailed_coords']['start'] + results['detailed_coords']['end']:\n",
    "       all_distances.append(coord['distance'])\n",
    "   \n",
    "   if all_distances:\n",
    "       percentiles = [25, 50, 75, 90, 95, 99]\n",
    "       print(\"Distance distribution\")\n",
    "       for p in percentiles:\n",
    "           value = np.percentile(all_distances, p)\n",
    "           print(f\"{p} percentile: {value:.2f} meters\")\n",
    "   \n",
    "   resolution_analysis = {}\n",
    "   for coord in results['detailed_coords']['start'] + results['detailed_coords']['end']:\n",
    "       original_res = h3.get_resolution(coord['original_h3'])\n",
    "       generalized_res = h3.get_resolution(coord['generalized_h3'])\n",
    "       \n",
    "       key = f\"{original_res}→{generalized_res}\"\n",
    "       if key not in resolution_analysis:\n",
    "           resolution_analysis[key] = []\n",
    "       resolution_analysis[key].append(coord['distance'])\n",
    "   \n",
    "   print(\"Resolution changes\")\n",
    "   for resolution_change, distances in resolution_analysis.items():\n",
    "       mean_dist = np.mean(distances)\n",
    "       count = len(distances)\n",
    "       print(f\"{resolution_change}: {mean_dist:.2f}m (n={count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368da582-55e4-4740-83c0-038bf222a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_results = calculate_generalization_distance_metric(\n",
    "   df=filtered_df, \n",
    "   od_matrix_generalized=od_matrix_generalized\n",
    ")\n",
    "\n",
    "analyze_generalization_impact(distance_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee39ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralizationMetric:\n",
    "    \"\"\"\n",
    "    Ḡ = (1/V+) × Σ(|o| + |d|) × v_{o→d}\n",
    "    \"\"\"\n",
    "    def __init__(self, k_threshold: int = 10):\n",
    "        self.k_threshold = k_threshold\n",
    "\n",
    "    def calculate_generalization_error(self, od_matrix_generalized: pd.DataFrame, od_matrix: pd.DataFrame) -> float:\n",
    "        # generalized -> number of original cells\n",
    "        origin_counts = self._build_hexagon_counts(\n",
    "            od_matrix_generalized, od_matrix, column_gen=\"start_gen\", column_orig=\"start_h3\"\n",
    "        )\n",
    "        destination_counts = self._build_hexagon_counts(\n",
    "            od_matrix_generalized, od_matrix, column_gen=\"end_gen\", column_orig=\"end_h3\"\n",
    "        )\n",
    "\n",
    "        total_volume_anonymous = 0\n",
    "        weighted_count_sum = 0\n",
    "\n",
    "        for _, row in od_matrix_generalized.iterrows():\n",
    "            flow_value = row[\"count\"]\n",
    "            if flow_value >= self.k_threshold:\n",
    "                origin_h3 = row[\"start_gen\"]\n",
    "                dest_h3   = row[\"end_gen\"]\n",
    "\n",
    "                origin_count = origin_counts.get(origin_h3, 1)\n",
    "                dest_count   = destination_counts.get(dest_h3, 1)\n",
    "\n",
    "                total_volume_anonymous += flow_value\n",
    "                weighted_count_sum += (origin_count + dest_count) * flow_value\n",
    "\n",
    "        return weighted_count_sum / total_volume_anonymous if total_volume_anonymous > 0 else 0.0\n",
    "\n",
    "    def _build_hexagon_counts(\n",
    "        self, od_matrix_generalized: pd.DataFrame, od_matrix: pd.DataFrame, \n",
    "        column_gen: str, column_orig: str\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Count how many original hexagons belong to each generalized hexagon.\n",
    "        \"\"\"\n",
    "        generalized_hexagons = od_matrix_generalized[column_gen].unique()\n",
    "        original_hexagons = od_matrix[column_orig].unique()\n",
    "\n",
    "        counts = {}\n",
    "        for gen_hex in generalized_hexagons:\n",
    "            target_res = h3.get_resolution(gen_hex)\n",
    "\n",
    "            # Find all parents of originals at target resolution\n",
    "            parent_series = [h3.cell_to_parent(h, target_res) for h in original_hexagons]\n",
    "\n",
    "            # Count how many times the parent == gen_hex appears\n",
    "            count = sum(1 for p in parent_series if p == gen_hex)\n",
    "            counts[gen_hex] = max(count, 1)  # fallback to 1\n",
    "\n",
    "        return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f081205b-807b-475b-8a08-ffb9256e10fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = GeneralizationMetric(k_threshold=10)\n",
    "error = metric.calculate_generalization_error(od_matrix_generalized, od_matrix)\n",
    "print(f\"Average generalization error Ḡ: {error:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd74c37-13ea-4e9e-92c1-c0229f3302d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_reconstruction_loss(original_od_df: pd.DataFrame,\n",
    "                             od_matrix_generalized: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    reconstruction loss:first_seen\n",
    "    E = (1/V) * Σ |ṽ_o→d - v_o→d|\n",
    "    \"\"\"\n",
    "    \n",
    "    # Crea un dizionario per accesso veloce ai flussi generalizzati\n",
    "    generalized_flows = {\n",
    "        (row['start_gen'], row['end_gen']): row['count']\n",
    "        for _, row in od_matrix_generalized.iterrows()\n",
    "    }\n",
    "    \n",
    "    total_volume = 0\n",
    "    total_abs_error = 0\n",
    "\n",
    "    gen_start_hexes = od_matrix_generalized['start_gen'].unique()\n",
    "    gen_end_hexes   = od_matrix_generalized['end_gen'].unique()\n",
    "\n",
    "    for _, row in original_od_df.iterrows():\n",
    "        start_h3 = row['start_h3']\n",
    "        end_h3   = row['end_h3']\n",
    "        true_count = row['count']\n",
    "        \n",
    "        # Trova gli esagoni generalizzati corrispondenti\n",
    "        gen_start = _find_generalized_parent(start_h3, gen_start_hexes)\n",
    "        gen_end   = _find_generalized_parent(end_h3, gen_end_hexes)\n",
    "        \n",
    "        if gen_start is None or gen_end is None:\n",
    "            continue\n",
    "            \n",
    "        # Search for the corresponding generalized flow\n",
    "        gen_key = (gen_start, gen_end)\n",
    "        gen_count = generalized_flows.get(gen_key, 0)\n",
    "        \n",
    "        # Calculate the absolute error\n",
    "        total_abs_error += abs(gen_count - true_count)\n",
    "        total_volume += true_count\n",
    "\n",
    "    return total_abs_error / total_volume if total_volume > 0 else 0.0\n",
    "\n",
    "\n",
    "def _find_generalized_parent(original_h3: str, generalized_hexagons: list) -> str:\n",
    "    \"\"\"\n",
    "    Find the generalized hexagon that contains the original hexagon.\n",
    "    \"\"\"\n",
    "    original_res = h3.get_resolution(original_h3)\n",
    "    \n",
    "    for gen_hex in generalized_hexagons:\n",
    "        gen_res = h3.get_resolution(gen_hex)\n",
    "        \n",
    "        if gen_res <= original_res:\n",
    "            parent = h3.cell_to_parent(original_h3, gen_res)\n",
    "            if parent == gen_hex:\n",
    "                return gen_hex\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4ff7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = fast_reconstruction_loss(\n",
    "    original_od_df=od_matrix,\n",
    "    od_matrix_generalized=od_matrix_generalized\n",
    ")\n",
    "print(f\"Reconstruction Loss: {loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35d748b",
   "metadata": {},
   "source": [
    "### Metrics with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "996449c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_discernability_and_cavg(df: pd.DataFrame, k: int, suppressed_count: int = 0) -> dict:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        df: DataFrame with ['start_h3', 'end_h3', 'count']\n",
    "        k: for k-anonimity\n",
    "        suppressed_count: number of OD pairs suppressed (optional)\n",
    "    \n",
    "    Returns:\n",
    "        dict con C_DM, C_AVG, total number of records and equivalence classes\n",
    "    \"\"\"\n",
    "    counts = df['total_weight'].values\n",
    "    total_records = counts.sum() + suppressed_count\n",
    "    total_equiv_classes = len(counts) + suppressed_count\n",
    "    \n",
    "    k_anonymous_counts = counts[counts >= k]\n",
    "    c_dm_gen = np.sum(k_anonymous_counts**2)\n",
    "    \n",
    "    # Penalty for suppressed records\n",
    "    suppression_penalty = suppressed_count * counts.sum()  # o totale record, a seconda della definizione\n",
    "    c_dm = c_dm_gen + suppression_penalty\n",
    "    \n",
    "    # C_AVG: (total_records / total_equiv_classes) / k\n",
    "    c_avg = (total_records / total_equiv_classes) / k if total_equiv_classes > 0 else float('inf')\n",
    "    \n",
    "    return {\n",
    "        'C_DM': c_dm,\n",
    "        'C_AVG': c_avg,\n",
    "        'total_records': total_records,\n",
    "        'total_equivalence_classes': total_equiv_classes,\n",
    "        'k': k\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afca580-4dc6-464c-bd67-4e54c862df0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_discernability_and_cavg(od_matrix_generalized, k=10*media_peso, suppressed_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0244d43-98c5-4b79-a80b-040325b642c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralizationMetric:\n",
    "    \"\"\"\n",
    "    Ḡ = (1/V+) × Σ(|o| + |d|) × v_{o→d}\n",
    "    \"\"\"\n",
    "    def __init__(self, k_threshold: int = 10):\n",
    "        self.k_threshold = k_threshold\n",
    "\n",
    "    def calculate_generalization_error(self, od_matrix_generalized: pd.DataFrame, od_matrix: pd.DataFrame) -> float:\n",
    "        # generalized -> number of original cells\n",
    "        origin_counts = self._build_hexagon_counts(\n",
    "            od_matrix_generalized, od_matrix, column_gen=\"start_gen\", column_orig=\"start_h3\"\n",
    "        )\n",
    "        destination_counts = self._build_hexagon_counts(\n",
    "            od_matrix_generalized, od_matrix, column_gen=\"end_gen\", column_orig=\"end_h3\"\n",
    "        )\n",
    "\n",
    "        total_volume_anonymous = 0\n",
    "        weighted_count_sum = 0\n",
    "\n",
    "        for _, row in od_matrix_generalized.iterrows():\n",
    "            flow_value = row[\"total_weight\"]\n",
    "            if flow_value >= self.k_threshold:\n",
    "                origin_h3 = row[\"start_gen\"]\n",
    "                dest_h3   = row[\"end_gen\"]\n",
    "\n",
    "                origin_count = origin_counts.get(origin_h3, 1)\n",
    "                dest_count   = destination_counts.get(dest_h3, 1)\n",
    "\n",
    "                total_volume_anonymous += flow_value\n",
    "                weighted_count_sum += (origin_count + dest_count) * flow_value\n",
    "\n",
    "        return weighted_count_sum / total_volume_anonymous if total_volume_anonymous > 0 else 0.0\n",
    "\n",
    "    def _build_hexagon_counts(\n",
    "        self, od_matrix_generalized: pd.DataFrame, od_matrix: pd.DataFrame, \n",
    "        column_gen: str, column_orig: str\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Count how many original hexagons belong to each generalized hexagon.\n",
    "        \"\"\"\n",
    "        generalized_hexagons = od_matrix_generalized[column_gen].unique()\n",
    "        original_hexagons = od_matrix[column_orig].unique()\n",
    "\n",
    "        counts = {}\n",
    "        for gen_hex in generalized_hexagons:\n",
    "            target_res = h3.get_resolution(gen_hex)\n",
    "\n",
    "            # Trova tutti i parent degli originali alla risoluzione target\n",
    "            parent_series = [h3.cell_to_parent(h, target_res) for h in original_hexagons]\n",
    "\n",
    "            # Conta quante volte compare il parent == gen_hex\n",
    "            count = sum(1 for p in parent_series if p == gen_hex)\n",
    "            counts[gen_hex] = max(count, 1)  # fallback a 1\n",
    "\n",
    "        return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fa0bf1-cc63-4dcf-b49e-20e7613779c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = GeneralizationMetric(k_threshold=10*media_peso)\n",
    "error = metric.calculate_generalization_error(od_matrix_generalized, od_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba87ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_reconstruction_loss(original_od_df: pd.DataFrame,\n",
    "                             od_matrix_generalized: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    reconstruction loss:\n",
    "    E = (1/V) * Σ |ṽ_o→d - v_o→d|\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    generalized_flows = {\n",
    "        (row['start_gen'], row['end_gen']): row['total_weight']\n",
    "        for _, row in od_matrix_generalized.iterrows()\n",
    "    }\n",
    "    \n",
    "    total_volume = 0\n",
    "    total_abs_error = 0\n",
    "\n",
    "    gen_start_hexes = od_matrix_generalized['start_gen'].unique()\n",
    "    gen_end_hexes   = od_matrix_generalized['end_gen'].unique()\n",
    "\n",
    "    for _, row in original_od_df.iterrows():\n",
    "        start_h3 = row['start_h3']\n",
    "        end_h3   = row['end_h3']\n",
    "        true_count = row['total_weight']\n",
    "        \n",
    "        # Find the corresponding generalized hexagons\n",
    "        gen_start = _find_generalized_parent(start_h3, gen_start_hexes)\n",
    "        gen_end   = _find_generalized_parent(end_h3, gen_end_hexes)\n",
    "        \n",
    "        if gen_start is None or gen_end is None:\n",
    "            continue\n",
    "            \n",
    "        # Search for the corresponding generalized flow\n",
    "        gen_key = (gen_start, gen_end)\n",
    "        gen_count = generalized_flows.get(gen_key, 0)\n",
    "        \n",
    "        # the absolute error\n",
    "        total_abs_error += abs(gen_count - true_count)\n",
    "        total_volume += true_count\n",
    "\n",
    "    return total_abs_error / total_volume if total_volume > 0 else 0.0\n",
    "\n",
    "\n",
    "def _find_generalized_parent(original_h3: str, generalized_hexagons: list) -> str:\n",
    "    \"\"\"\n",
    "    Find the generalized hexagon that contains the original hexagon.\n",
    "    \"\"\"\n",
    "    original_res = h3.get_resolution(original_h3)\n",
    "    \n",
    "    for gen_hex in generalized_hexagons:\n",
    "        gen_res = h3.get_resolution(gen_hex)\n",
    "        \n",
    "        if gen_res <= original_res:\n",
    "            parent = h3.cell_to_parent(original_h3, gen_res)\n",
    "            if parent == gen_hex:\n",
    "                return gen_hex\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8ed2a9-c179-4584-b9af-e4755402aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = fast_reconstruction_loss(\n",
    "    original_od_df=od_matrix,\n",
    "    od_matrix_generalized=od_matrix_generalized\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509a6178-a578-403f-ae9d-25cac20a4c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb2557e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c462f5e-4534-4484-9174-a96836165a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
